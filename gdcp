#!/usr/bin/env python

import os
import sys
import signal
import logging
from logging import info, warning
import re
import subprocess
import datetime
import time
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
import random
import mimetypes
import httplib
import httplib2
import ssl
import apiclient.errors
import apiclient.http
import oauth2client.client
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

VERSION = "0.5.6"
PROJ = "gdcp"  # name of this project
CONFIG_DIR = os.path.join(os.environ["HOME"], "." + PROJ)
CHUNKSIZE = 2 ** 20 * 250  # 250 MiB chunks

log = logging.getLogger(PROJ)
googleapi = logging.getLogger("googleapiclient.discovery")
oauth2_client = logging.getLogger("oauth2client.client")
oauth2_util = logging.getLogger("oauth2client.util")

class Gdcp(object):
    def __init__(self, drive, **kwargs):
        self.drive = drive
        self.failures = {"HTTP": [], "MD5": []}
        self.exclude_patterns = kwargs.get("excludes", [])
        self.excludes = [re.compile(e) for e in self.exclude_patterns]
        self.include = kwargs.get("include", False)

        # Should folders be considered in exclude rules?
        # By default rules only apply to files
        self.exclude_folders = kwargs.get("exclude_folders", False)

    def failed(self):
        return bool(len(self.failures["HTTP"]) or len(self.failures["MD5"]))

    def print_failed(self):
        if len(self.failures["HTTP"]):
            log.warning("%i files failed to transfer:\n%s" %
                        (len(self.failures["HTTP"]), "\n".join(self.failures["HTTP"])))
        if len(self.failures["MD5"]):
            log.warning("%i files failed MD5 verification:\n%s" %
                        (len(self.failures["MD5"]), "\n".join(self.failures["MD5"])))

    def upload(self, **kwargs):
        files = []
        paths = kwargs.get("paths", [])
        if len(paths) > 1:
            # Custom title turned off if more than one file
            kwargs["title"] = None
        for local_file in kwargs.get("paths", []):
            parent = kwargs.get("parent", None)
            checksum = kwargs["checksum"]
            title = kwargs.get("title", None)
            f = GdcpFile(self, path=local_file, parent=parent, checksum=checksum, title=title)
            f.upload()
            files.append(f)
        return files

    def download(self, **kwargs):
        files = []
        for _id in kwargs.get("ids", []):
            f = GdcpFile(self, id=_id, checksum=kwargs["checksum"], root=kwargs["root"])
            f.download()
            files.append(f)
        return files

class GdcpFile(object):

    def __init__(self, gdcp, **kwargs):
        self.gdcp = gdcp
        self.drive = gdcp.drive

        self.id = find_id(kwargs.get("id", None))
        self.path = kwargs.get("path", None)
        self.title = kwargs.get("title", None)
        self.parent = kwargs.get("parent", None)
        self.check_checksum = kwargs.get("checksum", True)
        self.root = kwargs.get("root", None)

        if self.path:
            self.mimetype = guess_mimetype(self.path)
            self.fileSize = os.path.getsize(self.path)
        if self.title is None:
            self.title = title_from_path(self.path)

        self.retry_limit = 6
        self.retries = 0
        self.prev_progress = 0.00
        self.bytes_sent = 0
        self.bytes_received = 0
        self.fail_md5_flag = False
        self.fail_upload_flag = False
        self.fail_download_flag = False

    def upload(self):
        # Check for match against any exclude rules
        if not self._passes_excludes():
            return

        log.info("Uploading %s" % self.path)

        if self._is_folder():
            # Folder
            self._create_google_folder()
            allfiles = path_join(self.path, os.listdir(self.path))
            allfiles = [x for x in allfiles if not os.path.islink(x)]
            self.gdcp.upload(paths=allfiles, parent=self.id, checksum=self.check_checksum)
        else:
            # File
            media_body = self._create_media_body()
            body = self._create_body()
            t0 = datetime.datetime.now()

            response = None

            if self.fileSize == 0:
                # zero size files don't do resumable chunked uploads
                request = self.drive.auth.service.files().insert(body=body)
                while response is None:
                    try:
                        response = request.execute()
                    except (apiclient.errors.HttpError, KeyError, ssl.SSLError, httplib2.HttpLib2Error) as e:
                        # Create sensible error string
                        if hasattr(e, "resp"):
                            # apiclient.errors.HttpError has http response in resp
                            err_msg = "HTTP %i" % e.resp.status
                        else:
                            err_msg = "Error %s" % e
                        if self.retries < self.retry_limit:
                            log.warning("%s, restarting upload in %is" % (err_msg, self._delay()))
                            time.sleep(self._delay())
                            self.retries += 1
                            request = self.drive.auth.service.files().insert(body=body)
                        else:
                            # No more retries left, abort
                            log.warning("%s, aborting" % err_msg)
                            self._fail_upload()
                            break
            else:
                # File is not empty, do resumable chunked upload
                request = self.drive.auth.service.files().insert(body=body, media_body=media_body)
                while response is None:
                    t1 = datetime.datetime.now()
                    try:
                        # Attempt to upload one chunk
                        status, response = request.next_chunk()
                        if status:
                            # Successfully sent a chunk, but download not complete yet
                            # Keep track of progress
                            prev_bytes_sent = self.bytes_sent
                            self._register_upload_chunk(status, t1)
                            log.debug("Uploaded bytes %i-%i" % (prev_bytes_sent + 1, self.bytes_sent))
                    except (apiclient.errors.HttpError, KeyError, ssl.SSLError, httplib2.HttpLib2Error) as e:
                        # Chunk upload threw an error, retry or restart

                        # Create sensible error string
                        if hasattr(e, "resp"):
                            err_msg = "HTTP %i" % e.resp.status
                        else:
                            err_msg = "Error %s" % e

                        if hasattr(e, "resp") and e.resp.status in [500, 502, 503, 504]:
                            # Retry this chunk
                            if self.retries < self.retry_limit:
                                delay = 2 ** resumable_retries
                                log.warning("%s, retrying chunk in %is" % (err_msg, self._delay()))
                                time.sleep(self._delay())
                                self.retries += 1
                            else:
                                # No more retries left, abort
                                log.warning("%s, aborting" % err_msg)
                                self._fail_upload()
                                break
                        else:
                            # Restart upload
                            if self.retries < self.retry_limit:
                                log.warning("%s, restarting upload in %is" % (err_msg, self._delay()))
                                time.sleep(self._delay())
                                self.prev_progress = 0.00
                                self.bytes_sent = 0
                                self.retries += 1
                                request = self.drive.auth.service.files().insert(body=body, media_body=media_body)
                            else:
                                # No more retries left, abort
                                log.warning("%s, aborting" % err_msg)
                                self._fail_upload()
                                break

            self._add_response(response)
            if response:
                t2 = datetime.datetime.now()
                rate = calc_transfer_rate(t0, t2, self.fileSize)
                log.info("Uploaded 100.00%%.  %i bytes in %s (%.02fMB/s) %s" %
                    (self.fileSize, format_timedelta(t0, t2), rate, self.id))
                if self.check_checksum:
                    self._check_md5()
            if self.fail_upload_flag or self.fail_md5_flag:
                log.warning("Upload failed for %s" % self.path)

    def download(self):
        self._get_google_file_metadata()

        # Check for match against any exclude rules
        if not self._passes_excludes():
            return

        if not os.path.exists(self.root):
            create_local_folder(self.root)

        self.path = os.path.join(self.root, self.title)
        self.path = de_duplicate_path_name(self.path)

        if self._is_folder():
            # Folder
            log.info("Creating folder %s" % self.path)
            create_local_folder(self.path)
            query = "'%s' in parents and trashed = false" % self.id
            file_list = self.drive.ListFile({"q": query}).GetList()
            ids = [f["id"] for f in file_list]
            self.gdcp.download(ids=ids, checksum=self.check_checksum, root=self.path)
        else:
            # File
            log.info("Downloading %s, size = %i, md5 = %s" % 
                (self.path, self.fileSize, self.google_md5Checksum))

            t0 = datetime.datetime.now()
            bytes_start = 0
            bytes_end = min(max(self.fileSize - 1, 0), CHUNKSIZE - 1)

            with open(self.path, "wb") as fh:
                while self.bytes_received < self.fileSize:
                    t1 = datetime.datetime.now()
                    headers = {
                        "range": "bytes=%i-%i" % (bytes_start, bytes_end),
                        "content-type": "application/octet-stream"
                    }

                    try:
                        response, content = self.drive.auth.service._http.request(self.downloadUrl, method="GET", headers=headers)
                    except httplib.IncompleteRead as e:
                        if self.retries < self.retry_limit:
                            log.warning("Exception %s, retrying bytes %i-%i in %is" %
                                        (e, bytes_start, bytes_end, self._delay()))
                            time.sleep(self._delay())
                            self.retries += 1
                            continue
                        else:
                            log.warning("Exception %s, aborting" % e)
                            self._fail_download()
                            fh.close()
                            os.remove(self.path)
                            break

                    if response.status in [206, 200]:
                        fh.write(content)
                        log.debug("Downloaded bytes %i-%i with status %s" %
                            (bytes_start, bytes_end, response.status))
                        self.bytes_received += bytes_end - bytes_start + 1
                        self._register_download_chunk(t1, bytes_end - bytes_start + 1)
                        bytes_start = bytes_end + 1
                        bytes_end = min(max(self.fileSize - 1, 0), bytes_start + CHUNKSIZE - 1)
                        
                    else:
                        if self.retries < self.retry_limit:
                            log.warning("HTTP error %i, retrying bytes %i-%i in %is" %
                                        (response.status, bytes_start, bytes_end, self._delay()))
                            time.sleep(self._delay())
                            self.retries += 1
                        else:
                            log.warning("HTTP error %i, aborting" % response.status)
                            self._fail_download()
                            fh.close()
                            os.remove(self.path)
                            break

            t2 = datetime.datetime.now()

            if not self.fail_download_flag:
                if self.check_checksum:
                    if self.check_checksum:
                        self._check_md5()
            if self.fail_download_flag or self.fail_md5_flag:
                log.warning("Download failed for %s" % self.path)
            else:
                rate = calc_transfer_rate(t0, t2, self.fileSize)
                log.info("Downloaded 100.00%%.  %i bytes in %s (%.02fMB/s)" %
                 (self.fileSize, format_timedelta(t0, t2), rate))

    def _get_google_file_metadata(self):
        if not hasattr(self, "response"):
            response = self.drive.auth.service.files().get(fileId=self.id).execute()
        self._add_response(response)

    def _create_google_folder(self):
        body = self._create_body()
        log.debug("About to create folder %s" % body)
        response = self.drive.auth.service.files().insert(body=body).execute()
        log.info("Created folder %s" % self.title)
        self._add_response(response)

    def _add_response(self, response):
        """
        Add response for file metadata from Google Drive

        For response keys/values see return object of get()
        https://developers.google.com/resources/api-libraries/documentation/drive/v2/python/latest/drive_v2.files.html#get
        """
        self.response = response
        self.id = response["id"]
        self.title = response["title"]
        self.google_md5Checksum = response.get("md5Checksum", None)
        self.downloadUrl = response.get("downloadUrl", None)
        self.fileSize = int(response.get("fileSize", 0))
        self.mimetype = response["mimeType"]

    def _is_folder(self):
        return self.mimetype == "application/vnd.google-apps.folder"

    def _create_media_body(self):
        return apiclient.http.MediaFileUpload(self.path,
            chunksize=CHUNKSIZE, resumable=True, mimetype=self.mimetype)

    def _create_body(self):
        body = {"title": self.title}
        if self.mimetype is None:
            body["mimeType"] = "application/vnd.google-apps.folder"
        else:
            body["mimeType"] = self.mimetype
        if self.parent is not None:
            body["parents"] = [{"id": self.parent}]
        return body

    def _delay(self):
        return (2 ** self.retries) + random.random()

    def _register_download_chunk(self, t1, bytes_this_chunk):
        """
        Track progress through download
        """
        try:
            cur_progress = float(self.bytes_received) / self.fileSize * 100
        except ZeroDivisionError:
            cur_progress = 100.00
        # Only print progress every 5% or greater
        if cur_progress - self.prev_progress > 5.0 and cur_progress < 100.00:
            t_tmp = datetime.datetime.now()
            rate = calc_transfer_rate(t1, t_tmp, bytes_this_chunk)
            log.info("Downloaded %.02f%% (%.02fMB/s)" %
                     (cur_progress, rate))
            self.prev_progress = cur_progress

    def _register_upload_chunk(self, status, t1):
        """
        Track progress through upload
        """
        cur_progress = status.progress() * 100
        self.bytes_sent = min(self.bytes_sent + CHUNKSIZE, self.fileSize)
        # Only print progress every 5% or greater
        diff_progress = cur_progress - self.prev_progress
        if diff_progress > 5.0 and cur_progress < 100.00:
            t_tmp = datetime.datetime.now()
            rate = calc_transfer_rate(t1, t_tmp, CHUNKSIZE)
            log.info("Uploaded %.02f%% (%.02fMB/s)" % (cur_progress, rate))
            self.prev_progress = cur_progress

    def _check_md5(self):
        """
        Confirm that response MD5 from Google matches MD5 for local
        file
        """
        log.info("Calculating MD5 checksum for %s" % self.path)
        try:
            output = subprocess.check_output(["openssl", "md5", self.path],
                                             stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as e:
            error("MD5 calculation exited with an error: '%s'" %
                  output.rstrip())
        self.local_md5Checksum = output.split()[-1]
        if self.local_md5Checksum == self.google_md5Checksum:
            log.info("MD5 OK.  %s (local) == %s" %
                     (self.local_md5Checksum, self.google_md5Checksum))
            return True
        else:
            log.warning("MD5 failed.  %s (local) != %s" %
                        (self.local_md5Checksum, self.google_md5Checksum))
            self._fail_md5()
            return False

    def _passes_excludes(self):
        """
        Check if file title passes exclude rules
        """
        if self._is_folder():
            if not self.gdcp.exclude_folders:
                # By default folders always pass
                return True
            # If exlude_folders is True, then apply rules even if this is a
            # folder

        if self.gdcp.include:
            passed = False
        else:
            passed = True
        for regex in self.gdcp.excludes:
            if regex.match(self.title):
                passed = not passed
                break
        return passed

    def _fail_md5(self):
        self.fail_md5_flag = True
        self.gdcp.failures["MD5"].append(self)

    def _fail_upload(self):
        self.fail_upload_flag = True
        self.gdcp.failures["HTTP"].append(self)

    def _fail_download(self):
        self.fail_download_flag = True
        self.gdcp.failures["HTTP"].append(self)


def main():
    configure_logging()
    configure_signals()
    do_setup()
    cli()

# -----------------------------------------------------------------------------
# Configuration functions
# -----------------------------------------------------------------------------
def configure_logging():
    """
    Configure logging handlers
    """
    fmt = "%(asctime)-25s %(levelname)-10s %(name)-26s: %(message)s"
    datefmt = "%m/%d/%Y %I:%M:%S %p"
    formatter = logging.Formatter(fmt=fmt, datefmt=datefmt)

    # Logger instances have been created globally
    google_handler = logging.StreamHandler()
    google_handler.setFormatter(formatter)
    googleapi.addHandler(google_handler)
    oauth2_client.addHandler(google_handler)
    oauth2_util.addHandler(google_handler)
    googleapi.setLevel(logging.WARNING)
    oauth2_client.setLevel(logging.WARNING)
    # oauth2_util is spitting out warnings like this
    # new_request() takes at most 1 positional argument (6 given)
    # set to error here to ignore
    oauth2_util.setLevel(logging.ERROR)

    main_handler = logging.StreamHandler()
    main_handler.setFormatter(formatter)
    log.addHandler(main_handler)
    log.setLevel(logging.INFO)

def verbose():
    googleapi.setLevel(logging.INFO)
    oauth2_client.setLevel(logging.INFO)
    oauth2_util.setLevel(logging.INFO)
    log.setLevel(logging.DEBUG)

def configure_signals():
    """
    Signal handling

    Reset SIGINT and SIGPIPE to defaults

    Don't raise KeyboardInterrupt exception on SIGINT
    Dont' raise IOError on SIGPIPE
    """
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)

def authorize(location=CONFIG_DIR):
    # Authentication
    settings_file = os.path.join(location, "settings.yaml")
    gauth = GoogleAuth(settings_file)
    gauth.CommandLineAuth()
    gauth.Authorize()
    return gauth

def create_GoogleDrive():
    return GoogleDrive(authorize())

def do_setup(location=CONFIG_DIR):
    settings = os.path.join(location, "settings.yaml")
    client_secrets = os.path.join(location, "client_secrets.json")
    credentials = os.path.join(location, "credentials.json")

    settings_text = ["client_config_file: %s" % client_secrets]
    settings_text.append("get_refresh_token: True")
    settings_text.append("save_credentials: True")
    settings_text.append("save_credentials_backend: file")
    settings_text.append("save_credentials_file: %s" % credentials)

    if not os.path.exists(location):
        os.mkdir(location)
    elif not os.path.isdir(location):
        error("~/.%s already exists and is a file" % PROJ)

    if not os.path.exists(settings):
        try:
            with open(settings, "w") as fh:
                fh.write("\n".join(settings_text) + "\n")
        except (OSError, IOError) as e:
            error("Could not create settings file %s.  %s" %
                  (settings, e))

    if not os.path.exists(client_secrets):
        msg = ["%s not present" % client_secrets, ""]
        msg.append("- Visit https://console.developers.google.com/")
        msg.append("- Create a new project and select it")
        msg.append("- Under 'APIs' make sure the 'Drive API' is turned on")
        msg.append("- Under 'Credentials' create a new OAuth client ID")
        msg.append("  Choose 'Installed -> Other' for application type")
        msg.append("- Click 'Download JSON' to download the secrets file")
        msg.append("- Copy the secrets file to %s" % client_secrets)
        error("\n".join(msg))

    # If credentials have not been obtained, do so now
    if not os.path.exists(credentials):
        authorize()

# -----------------------------------------------------------------------------
# Google Drive interaction functions
# -----------------------------------------------------------------------------
def list_all_files(drive, metadata):
    """
    Print metadata for all files in Google Drive
    """
    query = "trashed = false"
    log.debug("query = '%s'" % query)
    files = drive.ListFile({"q": query}).GetList()
    for afile in files:
        print_one_file(afile, metadata)

def list_root(drive, metadata, depth):
    """
    Print metadata for files in root of Google Drive
    """
    query = "trashed = false and 'root' in parents"
    log.debug("query = '%s'" % query)
    files = drive.ListFile({"q": query}).GetList()
    for subfile in files:
        print_one_file(subfile, metadata)
        if is_google_folder(subfile):
            list_file(drive, subfile, metadata, depth-1)

def list_file_by_id(drive, file_ids, metadata, depth):
    """
    Print metadata for a file given by ID
    """
    for file_id in file_ids:
        file_id = find_id(file_id)
        if file_id:
            afile = drive.CreateFile({"id": file_id})
            list_file(drive, afile, metadata, depth)

def list_file(drive, curfile, metadata, depth):
    """
    Print file listings starting at and including curfile.

    If depth == 0, only print this file. If depth != 0 and afile is a folder,
    call list_file on each child with depth-1. Essentially, descend to the bottom
    of the directory tree if depth < 0, stop if depth = 0, and continue for
    depth steps if depth > 0.
    """
    if depth >= 0:
        if is_google_folder(curfile):
            query = "trashed = false and '%s' in parents" % curfile["id"]
            log.debug("query = '%s'" % query)
            files = drive.ListFile({"q": query}).GetList()
            for subfile in files:
                print_one_file(subfile, metadata)
                if is_google_folder(subfile):
                    list_file(drive, subfile, metadata, depth-1)
        else:
            print_one_file(curfile, metadata)

# -----------------------------------------------------------------------------
# Utility Functions
# -----------------------------------------------------------------------------
def print_one_file(afile, metadata):
    """
    Print metadata for one file
    """

    line = []
    for meta in metadata:
        try:
            line.append("%s: %s" % (meta, remove_r_n(afile[meta])))
        except KeyError:
            # Some files don't have common properties
            # e.g. folders don't have a fileSize
            # We don't want to throw an error for these cases
            # error("Invalid metadata property " +
            #       "'%s' in -m string '%s' " % (meta, metadata) +
            #       "for file %s" % afile['title'])
            pass
    print ", ".join(line)

def remove_r_n(some_string):
    some_string = some_string.replace("\r", "^M")
    some_string = some_string.replace("\n", "^M")
    return some_string

def guess_mimetype(file_path):
    if os.path.isdir(file_path):
        mimetype = "application/vnd.google-apps.folder"
    else:
        mimetype = mimetypes.guess_type(file_path)[0]
        if mimetype is None:
            mimetype = "application/octet-stream"
    return mimetype

def path_join(join_path, files):
    """
    Return list of items in files joined to join_path.

    Useful if a common directory prefix needs to be joined to a list of files.
    """
    return map(lambda x: os.path.join(join_path, x), files)

def title_from_path(file_path):
    """
    Get a sanitized title for a file or folder.
    """
    if file_path is None:
        return None
    file_path = os.path.abspath(file_path)  # resolve things like /foo/../foo

    # Special case for root
    if file_path == "/":
        return "/"

    # Remove trailing "/"s
    i = len(file_path) - 1
    while file_path[i] == "/":
        i -= 1
    file_path = file_path[:i+1]

    title = file_path.split("/")[-1]
    return title

def de_duplicate_path_name(old_path):
    """
    Create a sensible new path name when old name is a duplicate.
    """
    if not os.path.exists(old_path):
        return old_path
        
    delimiter = "_duplicate_"
    old_path = os.path.normpath(old_path)
    head, tail = os.path.split(old_path)
    old_tail_atoms = tail.split(delimiter)

    if delimiter in tail and old_tail_atoms[-1].isdigit():
        i = old_tail_atoms[-1]
        new_tail = tail[0:-len(i)] + str(int(i) + 1)
    else:
        new_tail = tail + delimiter + '1'

    new_path = os.path.join(head, new_tail)
   
    if os.path.exists(new_path):
        new_path = de_duplicate_path_name(new_path)
    
    return new_path

def create_local_folder(folder_path):
    """
    Create a local folder
    """
    try:
        os.makedirs(folder_path)
    except OSError as e:
        error("Could not create directory %s. Perhaps it already exists."\
              % folder_path)

def chdir(folder_path):
    try:
        os.chdir(folder_path)
    except OSError as e:
        error("Could not change to directory %s" % folder_path)

def error(msg):
    log.error(msg)
    sys.exit(1)

def format_timedelta(t1, t2):
    """
    Format time delta between t2 and t1 as "Nd:Nh:Nm:Ns"
    """
    delta = t2 - t1
    delta_s = delta.total_seconds()
    elapse = []

    days = delta_s / (24 * 60 * 60)
    if int(days) > 0:
        elapse.append("%id" % int(days))
        delta_s -= days * 24 * 60 * 60

    hours = delta_s / (60 * 60)
    if int(hours):
        elapse.append("%ih" % int(hours))
        delta_s -= hours * 60 * 60

    minutes = delta_s / 60
    if int(minutes):
        elapse.append("%im" % int(minutes))
        delta_s -= minutes * 60

    seconds = delta_s
    elapse.append("%.02fs" % seconds)

    return "".join(elapse)

def calc_transfer_rate(t1, t2, file_size):
    """
    Return transfer rate for file_size bytes between t1 and t2
    """
    delta = t2 - t1
    delta_s = delta.total_seconds()
    try:
        rate = float(file_size) / delta_s / 10**6  # MB/s
    except ZeroDivisionError:
        rate = 0.00
    return rate

def is_google_folder(afile):
    return afile["mimeType"] == "application/vnd.google-apps.folder"

def find_id(id_string):
    """
    Return file ID from common URLs or URL fragments which contain the ID.

    Valid inputs include:
    - an ID
    - part or all of the Google Drive browser URL for a folder
      e.g. https://drive.google.com/drive/#folders/0B01234567890123456789012345/0B01234567890123456789012346
      e.g. 987012345/0B01234567890123456789012346
    - the download link for a file or folder provided in the Google Drive
      browser interface from "Get Link" in a context menu
      e.g. https://drive.google.com/open?id=0B01234567890123456789012346&authuser=0
    """
    if id_string is None:
        return None
    get_link = re.compile("^https://drive.google.com/open\?id=([^&]+)&.*$")
    address_bar = re.compile("^.*/([^/]+)$")
    match = get_link.match(id_string)
    if not match:
        match = address_bar.match(id_string)
    if match:
        id_string = match.groups()[0]
    return id_string

def parse_id_args(id_args):
    id_strings = []
    for _id in id_args:
        if _id == "-":
            for line in sys.stdin:
                parts = line.rstrip().split()
                id_strings.extend(parts)
        else:
            id_strings.append(_id)
    return id_strings

# -----------------------------------------------------------------------------
# Command-line interface functions
# -----------------------------------------------------------------------------
def cli():
    """
    Parse command-line options.
    """
    parser = ArgumentParser(
        description="Google Drive command-line interface",
        formatter_class=ArgumentDefaultsHelpFormatter)

    subparsers = parser.add_subparsers(
        dest="subcommand_name",
        help="Sub-command help")

    # Version
    parser_version = subparsers.add_parser(
        "version", 
        help="Print the semantic version number",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_version.set_defaults(func=cli_version)

    # List
    parser_list = subparsers.add_parser(
        "list", 
        help="List files in Google Drive. Trashed files are listed.",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_list.add_argument("--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_list.add_argument(
        "-m", "--metadata",
        default="title,fileSize,id,md5Checksum",
        help="""Comma separated list of metadata keys to display.
             See
             https://developers.google.com/drive/v2/reference/files#resource""")
    parser_list.add_argument(
        "-d", "--depth",
        default=0,
        type=int,
        help="""Depth of a recursive listing of files in a folder. 0 only
             lists the file or folder itself. Depth must be >= 0. Using a large
             -d value can be slow for deeply nested directory hierarchies with many
             files.""")
    parser_list.add_argument(
        "-a", "--all",
        default=False,
        action="store_true",
        help="""List all files except for trashed. Much faster than specifying
                a -d value < 0.""")
    parser_list.add_argument(
        "-i", "--id",
        default=[],
        action="append",
        help="""File or folder ID. If not specified listing starts at Google
                Drive root. - to read a list of IDs from STDIN.""")
    parser_list.set_defaults(func=cli_list_files)

    # Download
    parser_download = subparsers.add_parser(
        "download", 
        help="Download a file from Google Drive",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_download.add_argument(
        "--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_download.add_argument(
        "-i", "--id",
        default=[],
        action="append",
        help="File ID. - to read a list of IDs from STDIN.")
    parser_download.add_argument(
        "-n", "--no_checksum",
        default=False,
        action="store_true",
        help="Skip MD5 checksum verification after download")
    parser_download.add_argument(
        "-e", "--excludes",
        default=[],
        action="append",
        help="""Files with titles matching these Python regular expression
             pattern will be excluded. Does not apply to folders. (See
             --exclude_folders).""")
    parser_download.add_argument(
        "-v", "--invert_excludes",
        default=False,
        action="store_true",
        help="""Change exclude rules to become include rules""")
    parser_download.add_argument(
        "--exclude_folders",
        action="store_true",
        default=False,
        help="""Apply exclude rules to folder titles in addition to file titles""")
    parser_download.add_argument(
        "target",
        help="Destination directory")
    parser_download.set_defaults(func=cli_download_file)

    # Upload
    parser_upload = subparsers.add_parser(
        "upload", 
        help="Upload a file or folder to Google Drive",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_upload.add_argument(
        "--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_upload.add_argument(
        "-p", "--parent",
        metavar="ID",
        help="""Parent ID, i.e. containing folder ID. If not specified
             file will be placed in root folder.""")
    parser_upload.add_argument(
        "-n", "--no_checksum",
        default=False,
        action="store_true",
        help="Skip MD5 checksum verification after upload")
    parser_upload.add_argument(
        "-t", "--title",
        help="""Title for file/folder. Must be specified if folder is '.' or
             '..'""")
    parser_upload.add_argument(
        "-e", "--excludes",
        default=[],
        action="append",
        help="""Files with titles matching these Python regular expression
             pattern will be excluded. Does not apply to folders. (See
             --exclude_folders).""")
    parser_upload.add_argument(
        "-v", "--invert_excludes",
        default=False,
        action="store_true",
        help="""Change exclude rules to become include rules""")
    parser_upload.add_argument(
        "--exclude_folders",
        default=False,
        action="store_true",
        help="""Apply exclude rules to folder titles in addition to file titles""")
    parser_upload.add_argument(
        "files",
        nargs="+",
        help="Files/folders to upload")
    parser_upload.set_defaults(func=cli_upload_file)

    args = parser.parse_args()
    if args.subcommand_name != "version":
        args.drive = create_GoogleDrive()  # add GoogleDrive
        if args.verbose:
            verbose()
    args.func(args)

def cli_list_files(args):
    metadata_list = args.metadata.split(",")
    if args.depth < 0:
        error("list -d must be >= 0")
    if args.id is None:  # start at root
        if args.all:
            list_all_files(args.drive, metadata_list)
        else:
            list_root(args.drive, metadata_list, args.depth)
    else:
        if args.all:
            error("list -a not compatible with -d")
        ids = parse_id_args(args.id)
        list_file_by_id(args.drive, ids, metadata_list, args.depth)

def cli_download_file(args):
    gdcp = Gdcp(args.drive, excludes=args.excludes, include=args.invert_excludes,
        exclude_folders=args.exclude_folders)
    ids = parse_id_args(args.id)
    files = gdcp.download(ids=ids, checksum=not args.no_checksum, root=args.target)
    if gdcp.failed():
        gdcp.print_failed()
        sys.exit(1)
    else:
        log.info("All downloads were successful")

def cli_upload_file(args):
    gdcp = Gdcp(args.drive, excludes=args.excludes, include=args.invert_excludes,
        exclude_folders=args.exclude_folders)
    for f in args.files:
        if os.path.islink(f):
            error("Uploading symbolic links not supported")
    files = gdcp.upload(paths=args.files, title=args.title, parent=args.parent,
        checksum=not args.no_checksum)
    if gdcp.failed():
        gdcp.print_failed()
        sys.exit(1)
    else:
        log.info("All uploads were successful")

def cli_version(args):
    print("%s version %s" % (PROJ, VERSION))

if __name__ == "__main__":
    main()

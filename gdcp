#!/usr/bin/env python

import os
import sys
import signal
import logging
from logging import info, warning
import re
import subprocess
import datetime
import time
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
import random
import mimetypes
import httplib
import httplib2
import socket
import ssl
import apiclient.errors
import apiclient.http
import oauth2client.client
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
import backoff

VERSION = "0.6.1"
PROJ = "gdcp"  # name of this project
CONFIG_DIR = os.path.join(os.environ["HOME"], "." + PROJ)
CHUNKSIZE = 2 ** 20 * 250  # 250 MiB chunks

log = logging.getLogger(PROJ)
googleapi = logging.getLogger("googleapiclient.discovery")
oauth2_client = logging.getLogger("oauth2client.client")
oauth2_util = logging.getLogger("oauth2client.util")
backoff_log = logging.getLogger('backoff')

class Gdcp(object):
    def __init__(self, drive, excludes=[], include=False, exclude_folders=False):
        self.drive = drive
        self.excludes = [re.compile(e) for e in excludes]
        self.include = include
        # Should folders be considered in exclude rules?
        # By default rules only apply to files
        self.exclude_folders = exclude_folders

        self.files = []
        self.failures = {"HTTP": [], "MD5": []}

        # https://developers.google.com/resources/api-libraries/documentation/drive/v2/python/latest/drive_v2.about.html
        #self.about = drive.auth.service.about().get().execute()

    def failed(self):
        return bool(len(self.failures["HTTP"]) or len(self.failures["MD5"]))

    def print_failed(self):
        if len(self.failures["HTTP"]):
            log.warning("%i files failed to transfer:\n%s" %
                        (len(self.failures["HTTP"]), "\n".join(self.failures["HTTP"])))
        if len(self.failures["MD5"]):
            log.warning("%i files failed MD5 verification:\n%s" %
                        (len(self.failures["MD5"]), "\n".join(self.failures["MD5"])))

    def upload(self, paths=[], title=None, parent="root", checksum=True):
        if len(paths) > 1:
            title = None  # custom title turned off if more than one file
        for local_file in paths:
            f = GdcpFile(self, path=local_file, parent=parent, checksum=checksum, title=title)
            f.upload()

    def download(self, ids=[], checksum=True, root="."):
        for _id in ids:
            f = GdcpFile(self, gid=_id, checksum=checksum, root=root)
            f.download()

    def list(self, ids=[], metadata=[], depth=0):
        for _id in ids:
            f = GdcpFile(self, gid=_id)
            f.list(metadata=metadata, depth=depth)

    def transfer_ownership(self, ids, email, permId=None):
        for _id in ids:
            f = GdcpFile(self, gid=_id)
            f.transfer_ownership(email, root=True)

    def list_all_files(self, metadata=[]):
        """
        Print metadata for all files in Google Drive
        """
        query = "trashed = false"
        log.debug("query = '%s'" % query)
        request = self.drive.auth.service.files().list(q=query, maxResults=460)
        while request != None:
            response = excute_request(request)
            for i in response["items"]:
                g = GdcpFile(self, metadata=i)
                g.print_metadata(metadata)
            request = self.drive.auth.service.files().list_next(request, response)

class GdcpFile(object):

    def __init__(self, gdcp, gid=None, path=None, title=None, parent=None,
        checksum=True, root=None, metadata=None):
        self.gdcp = gdcp
        self.drive = gdcp.drive

        self.id = find_id(gid)
        self.path = path
        self.title = title
        self.parent = find_id(parent)
        self.check_checksum = checksum
        self.root = root

        if self.path:
            self.mimetype = guess_mimetype(self.path)
            self.fileSize = os.path.getsize(self.path)
        if self.title is None:
            self.title = title_from_path(self.path)

        self._metadata = metadata

        self.retry_limit = 6
        self.prev_progress = 0.00
        self.bytes_sent = 0
        self.bytes_received = 0
        self.fail_md5_flag = False
        self.fail_upload_flag = False
        self.fail_download_flag = False

    def upload(self):
        """
        Recursively upload a file/folder (self.path) to a Google Drive parent
        folder whose ID is self.parent.
        """
        # Check for match against any exclude rules
        if not self._passes_excludes():
            return

        log.info("Uploading %s" % self.path)

        if self._is_folder():
            # Folder
            self._create_google_folder()
            self.gdcp.files.append(self)  # Add this folder to Gdcp object
            allfiles = path_join(self.path, os.listdir(self.path))
            allfiles = [x for x in allfiles if not os.path.islink(x)]
            self.gdcp.upload(paths=allfiles, parent=self.id, checksum=self.check_checksum)
        else:
            # File
            media_body = self._create_media_body()
            body = self._create_body()
            t0 = datetime.datetime.now()

            retries = 0
            response = None

            if self.fileSize == 0:
                # zero size files don't do resumable chunked uploads
                request = self.drive.auth.service.files().insert(body=body)
                while response is None:
                    try:
                        response = request.execute()
                    except (apiclient.errors.HttpError, KeyError, ssl.SSLError, 
                            httplib2.HttpLib2Error, httplib.BadStatusLine) as e:
                        # Create sensible error string
                        if hasattr(e, "resp"):
                            # apiclient.errors.HttpError has http response in resp
                            err_msg = "HTTP %i" % e.resp.status
                        else:
                            err_msg = "Error %s" % e
                        if retries < self.retry_limit:
                            log.warning("%s, restarting upload in %is" % (err_msg, delay(retries)))
                            time.sleep(delay(retries))
                            retries += 1
                            request = self.drive.auth.service.files().insert(body=body)
                        else:
                            # No more retries left, abort
                            log.warning("%s, aborting" % err_msg)
                            self._fail_upload()
                            break
            else:
                # File is not empty, do resumable chunked upload
                request = self.drive.auth.service.files().insert(body=body, media_body=media_body)
                while response is None:
                    t1 = datetime.datetime.now()
                    try:
                        # Attempt to upload one chunk
                        status, response = request.next_chunk()
                        if status:
                            # Successfully sent a chunk, but download not complete yet
                            # Keep track of progress
                            prev_bytes_sent = self.bytes_sent
                            self._register_upload_chunk(status, t1)
                            log.debug("Uploaded bytes %i-%i" % (prev_bytes_sent + 1, self.bytes_sent))
                    except (apiclient.errors.HttpError, KeyError, ssl.SSLError,
                            httplib2.HttpLib2Error, httplib.BadStatusLine) as e:
                        # Chunk upload threw an error, retry or restart

                        # Create sensible error string
                        if hasattr(e, "resp"):
                            err_msg = "HTTP %i" % e.resp.status
                        else:
                            err_msg = "Error %s" % e

                        if hasattr(e, "resp") and e.resp.status in [500, 502, 503, 504]:
                            # Retry this chunk
                            if retries < self.retry_limit:
                                log.warning("%s, retrying chunk in %is" % (err_msg, delay(retries)))
                                time.sleep(delay(retries))
                                retries += 1
                            else:
                                # No more retries left, abort
                                log.warning("%s, aborting" % err_msg)
                                self._fail_upload()
                                break
                        else:
                            # Restart upload
                            if retries < self.retry_limit:
                                log.warning("%s, restarting upload in %is" % (err_msg, delay(retries)))
                                time.sleep(delay(retries))
                                self.prev_progress = 0.00
                                self.bytes_sent = 0
                                retries += 1
                                request = self.drive.auth.service.files().insert(body=body, media_body=media_body)
                            else:
                                # No more retries left, abort
                                log.warning("%s, aborting" % err_msg)
                                self._fail_upload()
                                break

            self.metadata = response
            if response:
                t2 = datetime.datetime.now()
                rate = calc_transfer_rate(t0, t2, self.fileSize)
                log.info("Uploaded 100.00%%.  %i bytes in %s (%.02fMB/s) %s" %
                    (self.fileSize, format_timedelta(t0, t2), rate, self.id))
                if self.check_checksum:
                    self._check_md5()
            if self.fail_upload_flag or self.fail_md5_flag:
                log.warning("Upload failed for %s" % self.path)
            else:
                self.gdcp.files.append(self)  # Add this file to Gdcp object

    def download(self):
        """
        Recursively download a file/folder to local filesystem starting
        at self.root.
        """
        # Make sure file metadata is present
        self._ensure_google_file_metadata()

        # Check for match against any exclude rules
        if not self._passes_excludes():
            return

        if not os.path.exists(self.root):
            create_local_folder(self.root)

        self.path = os.path.join(self.root, self.title)
        self.path = de_duplicate_path_name(self.path)

        if self._is_folder():
            # Folder
            log.info("Creating folder %s" % self.path)
            create_local_folder(self.path)
            self.gdcp.files.append(self)  # Add this folder to Gdcp object
            children = self._get_children()
            for f in children:
                f.root = self.path # reset root to be this file's path
                f.download()
        else:
            # File
            log.info("Downloading %s, size = %i, md5 = %s" % 
                (self.path, self.fileSize, self.google_md5Checksum))

            t0 = datetime.datetime.now()
            retries = 0
            bytes_start = 0
            bytes_end = min(max(self.fileSize - 1, 0), CHUNKSIZE - 1)

            with open(self.path, "wb") as fh:
                while self.bytes_received < self.fileSize:
                    t1 = datetime.datetime.now()
                    headers = {
                        "range": "bytes=%i-%i" % (bytes_start, bytes_end),
                        "content-type": "application/octet-stream"
                    }

                    try:
                        response, content = self.drive.auth.service._http.request(self.downloadUrl, method="GET", headers=headers)
                    except httplib.IncompleteRead as e:
                        if retries < self.retry_limit:
                            log.warning("Exception %s, retrying bytes %i-%i in %is" %
                                        (e, bytes_start, bytes_end, delay(retries)))
                            time.sleep(delay(retries))
                            retries += 1
                            continue
                        else:
                            log.warning("Exception %s, aborting" % e)
                            self._fail_download()
                            fh.close()
                            os.remove(self.path)
                            break

                    if response.status in [206, 200]:
                        fh.write(content)
                        log.debug("Downloaded bytes %i-%i with status %s" %
                            (bytes_start, bytes_end, response.status))
                        self.bytes_received += bytes_end - bytes_start + 1
                        self._register_download_chunk(t1, bytes_end - bytes_start + 1)
                        bytes_start = bytes_end + 1
                        bytes_end = min(max(self.fileSize - 1, 0), bytes_start + CHUNKSIZE - 1)
                        
                    else:
                        if retries < self.retry_limit:
                            log.warning("HTTP error %i, retrying bytes %i-%i in %is" %
                                        (response.status, bytes_start, bytes_end, delay(retries)))
                            time.sleep(delay(retries))
                            retries += 1
                        else:
                            log.warning("HTTP error %i, aborting" % response.status)
                            self._fail_download()
                            fh.close()
                            os.remove(self.path)
                            break

            t2 = datetime.datetime.now()

            if not self.fail_download_flag:
                if self.check_checksum:
                    if self.check_checksum:
                        self._check_md5()
            if self.fail_download_flag or self.fail_md5_flag:
                log.warning("Download failed for %s" % self.path)
            else:
                rate = calc_transfer_rate(t0, t2, self.fileSize)
                log.info("Downloaded 100.00%%.  %i bytes in %s (%.02fMB/s)" %
                 (self.fileSize, format_timedelta(t0, t2), rate))
                self.gdcp.files.append(self)  # Add this file to Gdcp object

    def list(self, metadata, depth=0, indent=""):
        """
        Print file listings starting at and including self.

        If depth < -1 do nothing.
        If depth == -1, only print this file.
        If > -1 print this file and if this is a folder call list on each
        child with depth - 1.
        """
        if depth >= 0:
            self.print_metadata(metadata, indent)
            if self._is_folder():
                children = self._get_children()
                for c in children:
                    c.list(metadata, depth-1, indent=indent + "  ")
        elif depth == -1:
            self.print_metadata(metadata, indent)

    def print_metadata(self, metadata, indent=""):
        """
        Print metadata for this file
        """
        self._ensure_google_file_metadata()
        line = []
        for prop in metadata:
            try:
                line.append("%s: %s" % (prop, remove_r_n(self.metadata[prop])))
            except KeyError:
                # Some files don't have common properties
                # e.g. folders don't have a fileSize
                # We don't want to throw an error for these cases
                # error("Invalid metadata property " +
                #       "'%s' in -m string '%s' " % (meta, metadata) +
                #       "for file %s" % afile['title'])
                pass
        print indent + "\t".join(line)

    def transfer_ownership(self, email, root=True):
        """
        Transfer file ownership to new_owner_email.

        The account for new_owner_email must be in the same Google Apps domain.
        """
        self._ensure_google_file_metadata()

        body = {
            "type": "user",
            "value": email
        }
        if root:
            # Inserting an ownership permission for the top-level folder
            # will send an email and place the folder/file in the new owner's
            # My Drive root folder, unless they already have write permission.
            body["role"] = "owner"
            insert_resp = self.drive.auth.service.permissions().insert(
                fileId=self.id,
                body=body).execute()
        else:
            # First make sure writer permission is inserted. Suppress emails.
            # If we just insert an ownership permission here first, and there
            # wasn't already a write permission, the new owner will:
            # 1) receive an email for every file
            # 2) the file will be placed in their My Drive root folder. Neither is
            # desirable.
            body["role"] = "writer"
            insert_resp = self.drive.auth.service.permissions().insert(
                fileId=self.id,
                body=body,
                sendNotificationEmails=False).execute()
            log.debug("Granted write privileges for %s" % self.title)
            # Now update permission to owner role. Because the new owner
            # already had write permission an email is not sent and a
            # reference to the file is not placed in My Drive.
            permId = insert_resp["id"]
            body["role"] = "owner"
            owner_resp = None
            retries = 0
            while owner_resp is None:
                try:
                    owner_resp = self.drive.auth.service.permissions().update(
                        fileId=self.id,
                        permissionId=permId,
                        body=body,
                        transferOwnership=True).execute()
                except apiclient.errors.HttpError as e:
                    if retries < self.retry_limit:                    
                        log.debug("Could not update permissionId %s, retrying in %s" % (permId, delay(retries)))
                        # If the write permission was just inserted above
                        # it may not be available for updating, wait and try again
                        time.sleep(delay(retries))
                        retries += 1
                    else:
                        error("Could not get permission for permissionId %s" % permId)
        self.gdcp.files.append(self)  # Add this file to Gdcp object
        log.info("Transferred ownership of %s" % self.title)

        if self._is_folder():
            children = self._get_children()
            for f in children:
                f.transfer_ownership(email, root=False)

    @property
    def metadata(self):
        return self._metadata

    @metadata.setter
    def metadata(self, response):
        """
        Add response for file metadata from Google Drive

        For response keys/values see return object of get()
        https://developers.google.com/resources/api-libraries/documentation/drive/v2/python/latest/drive_v2.files.html#get
        """
        if response:
            self._metadata = response
            self.id = response["id"]
            self.title = response["title"]
            self.google_md5Checksum = response.get("md5Checksum", None)
            self.downloadUrl = response.get("downloadUrl", None)
            self.fileSize = int(response.get("fileSize", 0))
            self.mimetype = response["mimeType"]

    def _get_children(self):
        """
        Return list of GdcfFile objects for this file's children
        """
        children = []
        query = "trashed = false and '%s' in parents" % self.id
        request = self.drive.auth.service.files().list(q=query, maxResults=460)
        while request != None:
            response = execute_request(request)
            for i in response["items"]:
                g = GdcpFile(self.gdcp) # child inherits Gdcp object
                g.metadata = i
                g.check_checksum = self.check_checksum # child inherits check_checksum
                g.root = self.root # child inherits root
                children.append(g)
            request = self.drive.auth.service.files().list_next(request, response)
        return children

    def _ensure_google_file_metadata(self):
        """
        Ensure that file metadata from Google is present. Don't perform remote
        API call if file metadata is already present.
        """
        if not self.metadata and self.id:
            log.debug("Ensure fired for %s, %s" % (self.id, self.title))
            request = self.drive.auth.service.files().get(fileId=self.id)
            response = execute_request(request)
            self.metadata = response

    def _create_google_folder(self):
        body = self._create_body()
        log.debug("About to create folder %s" % body)
        request = self.drive.auth.service.files().insert(body=body)
        response = execute_request(request)
        # Sometimes folders take a while to become available. Try to get metadata
        # from Google to make sure it's available before moving on
        request = self.drive.auth.service.files().get(fileId=response["id"])
        response = execute_request(request)
        self.metadata = response
        log.info("Created folder %s %s" % (self.title, self.id))

    def _is_folder(self):
        self._ensure_google_file_metadata()
        return self.mimetype == "application/vnd.google-apps.folder"

    def _create_media_body(self):
        return apiclient.http.MediaFileUpload(self.path,
            chunksize=CHUNKSIZE, resumable=True, mimetype=self.mimetype)

    def _create_body(self):
        body = {"title": self.title}
        if self.mimetype is None:
            body["mimeType"] = "application/vnd.google-apps.folder"
        else:
            body["mimeType"] = self.mimetype
        if self.parent is not None:
            body["parents"] = [{"id": self.parent}]
        return body

    def _register_download_chunk(self, t1, bytes_this_chunk):
        """
        Track progress through download
        """
        try:
            cur_progress = float(self.bytes_received) / self.fileSize * 100
        except ZeroDivisionError:
            cur_progress = 100.00
        # Only print progress every 5% or greater
        if cur_progress - self.prev_progress > 5.0 and cur_progress < 100.00:
            t_tmp = datetime.datetime.now()
            rate = calc_transfer_rate(t1, t_tmp, bytes_this_chunk)
            log.info("Downloaded %.02f%% (%.02fMB/s)" %
                     (cur_progress, rate))
            self.prev_progress = cur_progress

    def _register_upload_chunk(self, status, t1):
        """
        Track progress through upload
        """
        cur_progress = status.progress() * 100
        self.bytes_sent = min(self.bytes_sent + CHUNKSIZE, self.fileSize)
        # Only print progress every 5% or greater
        diff_progress = cur_progress - self.prev_progress
        if diff_progress > 5.0 and cur_progress < 100.00:
            t_tmp = datetime.datetime.now()
            rate = calc_transfer_rate(t1, t_tmp, CHUNKSIZE)
            log.info("Uploaded %.02f%% (%.02fMB/s)" % (cur_progress, rate))
            self.prev_progress = cur_progress

    def _check_md5(self):
        """
        Confirm that response MD5 from Google matches MD5 for local
        file
        """
        log.info("Calculating MD5 checksum for %s" % self.path)
        try:
            output = subprocess.check_output(["openssl", "md5", self.path],
                                             stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as e:
            error("MD5 calculation exited with an error: '%s'" %
                  output.rstrip())
        self.local_md5Checksum = output.split()[-1]
        if self.local_md5Checksum == self.google_md5Checksum:
            log.info("MD5 OK.  %s (local) == %s" %
                     (self.local_md5Checksum, self.google_md5Checksum))
            return True
        else:
            log.warning("MD5 failed.  %s (local) != %s" %
                        (self.local_md5Checksum, self.google_md5Checksum))
            self._fail_md5()
            return False

    def _passes_excludes(self):
        """
        Check if file title passes exclude rules
        """
        if self._is_folder():
            if not self.gdcp.exclude_folders:
                # By default folders always pass
                return True
            # If exlude_folders is True, then apply rules even if this is a
            # folder

        if self.gdcp.include:
            passed = False
        else:
            passed = True
        for regex in self.gdcp.excludes:
            if regex.match(self.title):
                passed = not passed
                break
        return passed

    def _fail_md5(self):
        self.fail_md5_flag = True
        self.gdcp.failures["MD5"].append(self)

    def _fail_upload(self):
        self.fail_upload_flag = True
        self.gdcp.failures["HTTP"].append(self)

    def _fail_download(self):
        self.fail_download_flag = True
        self.gdcp.failures["HTTP"].append(self)


def main():
    configure_logging()
    configure_signals()
    do_setup()
    cli()

# -----------------------------------------------------------------------------
# Configuration functions
# -----------------------------------------------------------------------------
def configure_logging():
    """
    Configure logging handlers
    """
    fmt = "%(asctime)-25s %(levelname)-10s %(name)-26s: %(message)s"
    datefmt = "%m/%d/%Y %I:%M:%S %p"
    formatter = logging.Formatter(fmt=fmt, datefmt=datefmt)

    # Logger instances have been created globally
    google_handler = logging.StreamHandler()
    google_handler.setFormatter(formatter)
    googleapi.addHandler(google_handler)
    oauth2_client.addHandler(google_handler)
    oauth2_util.addHandler(google_handler)
    googleapi.setLevel(logging.WARNING)
    oauth2_client.setLevel(logging.WARNING)
    # oauth2_util is spitting out warnings like this
    # new_request() takes at most 1 positional argument (6 given)
    # set to error here to ignore
    oauth2_util.setLevel(logging.ERROR)
    backoff_handler = logging.StreamHandler()
    backoff_handler.setFormatter(formatter)
    backoff_log.addHandler(backoff_handler)

    main_handler = logging.StreamHandler()
    main_handler.setFormatter(formatter)
    log.addHandler(main_handler)
    log.setLevel(logging.INFO)

def verbose():
    googleapi.setLevel(logging.INFO)
    oauth2_client.setLevel(logging.INFO)
    oauth2_util.setLevel(logging.INFO)
    backoff_log.setLevel(logging.INFO)
    log.setLevel(logging.DEBUG)

def configure_signals():
    """
    Signal handling

    Reset SIGINT and SIGPIPE to defaults

    Don't raise KeyboardInterrupt exception on SIGINT
    Dont' raise IOError on SIGPIPE
    """
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)

def authorize(location=CONFIG_DIR):
    # Authentication
    settings_file = os.path.join(location, "settings.yaml")
    gauth = GoogleAuth(settings_file)
    gauth.CommandLineAuth()
    gauth.Authorize()
    return gauth

def create_GoogleDrive():
    return GoogleDrive(authorize())

def do_setup(location=CONFIG_DIR):
    settings = os.path.join(location, "settings.yaml")
    client_secrets = os.path.join(location, "client_secrets.json")
    credentials = os.path.join(location, "credentials.json")

    settings_text = ["client_config_file: %s" % client_secrets]
    settings_text.append("get_refresh_token: True")
    settings_text.append("save_credentials: True")
    settings_text.append("save_credentials_backend: file")
    settings_text.append("save_credentials_file: %s" % credentials)

    if not os.path.exists(location):
        os.mkdir(location)
    elif not os.path.isdir(location):
        error("~/.%s already exists and is a file" % PROJ)

    if not os.path.exists(settings):
        try:
            with open(settings, "w") as fh:
                fh.write("\n".join(settings_text) + "\n")
        except (OSError, IOError) as e:
            error("Could not create settings file %s.  %s" %
                  (settings, e))

    if not os.path.exists(client_secrets):
        msg = ["%s not present" % client_secrets, ""]
        msg.append("- Visit https://console.developers.google.com/")
        msg.append("- Create a new project and select it")
        msg.append("- Under 'APIs' make sure the 'Drive API' is turned on")
        msg.append("- Under 'Credentials' create a new OAuth client ID")
        msg.append("  Choose 'Installed -> Other' for application type")
        msg.append("- Click 'Download JSON' to download the secrets file")
        msg.append("- Copy the secrets file to %s" % client_secrets)
        error("\n".join(msg))

    # If credentials have not been obtained, do so now
    if not os.path.exists(credentials):
        authorize()

# -----------------------------------------------------------------------------
# Utility Functions
# -----------------------------------------------------------------------------
@backoff.on_exception(backoff.expo, apiclient.errors.HttpError, max_tries=6)
@backoff.on_exception(backoff.expo, httplib2.HttpLib2Error, max_tries=6)
@backoff.on_exception(backoff.expo, socket.error, max_tries=6)
def execute_request(request):
    return request.execute()

def delay(retries):
    return (2 ** retries) + random.random()

def remove_r_n(some_string):
    some_string = some_string.replace("\r", "^M")
    some_string = some_string.replace("\n", "^M")
    return some_string

def guess_mimetype(file_path):
    if os.path.isdir(file_path):
        mimetype = "application/vnd.google-apps.folder"
    else:
        mimetype = mimetypes.guess_type(file_path)[0]
        if mimetype is None:
            mimetype = "application/octet-stream"
    return mimetype

def path_join(join_path, files):
    """
    Return list of items in files joined to join_path.

    Useful if a common directory prefix needs to be joined to a list of files.
    """
    return map(lambda x: os.path.join(join_path, x), files)

def title_from_path(file_path):
    """
    Get a sanitized title for a file or folder.
    """
    if file_path is None:
        return None
    file_path = os.path.abspath(file_path)  # resolve things like /foo/../foo

    # Special case for root
    if file_path == "/":
        return "/"

    # Remove trailing "/"s
    i = len(file_path) - 1
    while file_path[i] == "/":
        i -= 1
    file_path = file_path[:i+1]

    title = file_path.split("/")[-1]
    return title

def de_duplicate_path_name(old_path):
    """
    Create a sensible new path name when old name is a duplicate.
    """
    if not os.path.exists(old_path):
        return old_path
        
    delimiter = "_duplicate_"
    old_path = os.path.normpath(old_path)
    head, tail = os.path.split(old_path)
    old_tail_atoms = tail.split(delimiter)

    if delimiter in tail and old_tail_atoms[-1].isdigit():
        i = old_tail_atoms[-1]
        new_tail = tail[0:-len(i)] + str(int(i) + 1)
    else:
        new_tail = tail + delimiter + '1'

    new_path = os.path.join(head, new_tail)
   
    if os.path.exists(new_path):
        new_path = de_duplicate_path_name(new_path)
    
    return new_path

def create_local_folder(folder_path):
    """
    Create a local folder
    """
    try:
        os.makedirs(folder_path)
    except OSError as e:
        error("Could not create directory %s. Perhaps it already exists."\
              % folder_path)

def chdir(folder_path):
    try:
        os.chdir(folder_path)
    except OSError as e:
        error("Could not change to directory %s" % folder_path)

def error(msg):
    log.error(msg)
    sys.exit(1)

def format_timedelta(t1, t2):
    """
    Format time delta between t2 and t1 as "Nd:Nh:Nm:Ns"
    """
    delta = t2 - t1
    delta_s = delta.total_seconds()
    elapse = []

    days = delta_s / (24 * 60 * 60)
    if int(days) > 0:
        elapse.append("%id" % int(days))
        delta_s -= days * 24 * 60 * 60

    hours = delta_s / (60 * 60)
    if int(hours):
        elapse.append("%ih" % int(hours))
        delta_s -= hours * 60 * 60

    minutes = delta_s / 60
    if int(minutes):
        elapse.append("%im" % int(minutes))
        delta_s -= minutes * 60

    seconds = delta_s
    elapse.append("%.02fs" % seconds)

    return "".join(elapse)

def calc_transfer_rate(t1, t2, file_size):
    """
    Return transfer rate for file_size bytes between t1 and t2
    """
    delta = t2 - t1
    delta_s = delta.total_seconds()
    try:
        rate = float(file_size) / delta_s / 10**6  # MB/s
    except ZeroDivisionError:
        rate = 0.00
    return rate

def is_google_folder(afile):
    return afile["mimeType"] == "application/vnd.google-apps.folder"

def find_id(id_string):
    """
    Return file ID from common URLs or URL fragments which contain the ID.

    Valid inputs include:
    - an ID
    - part or all of the Google Drive browser URL for a folder
      e.g. https://drive.google.com/drive/#folders/0B01234567890123456789012345/0B01234567890123456789012346
      e.g. 987012345/0B01234567890123456789012346
    - the download link for a file or folder provided in the Google Drive
      browser interface from "Get Link" in a context menu
      e.g. https://drive.google.com/open?id=0B01234567890123456789012346&authuser=0
    """
    if id_string is None:
        return None
    get_link = re.compile("^https://drive.google.com/open\?id=([^&]+)&.*$")
    address_bar = re.compile("^.*/([^/]+)$")
    match = get_link.match(id_string)
    if not match:
        match = address_bar.match(id_string)
    if match:
        id_string = match.groups()[0]
    return id_string

def parse_id_args(id_args):
    id_strings = []
    for _id in id_args:
        if _id == "-":
            for line in sys.stdin:
                parts = line.rstrip().split()
                id_strings.extend(parts)
        else:
            id_strings.append(_id)
    return id_strings

# -----------------------------------------------------------------------------
# Command-line interface functions
# -----------------------------------------------------------------------------
def cli():
    """
    Parse command-line options.
    """
    parser = ArgumentParser(
        description="Google Drive command-line interface",
        formatter_class=ArgumentDefaultsHelpFormatter)

    subparsers = parser.add_subparsers(
        dest="subcommand_name",
        help="Sub-command help")

    # Version
    parser_version = subparsers.add_parser(
        "version", 
        help="Print the semantic version number",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_version.set_defaults(func=cli_version)

    # List
    parser_list = subparsers.add_parser(
        "list", 
        help="List files in Google Drive. Trashed files are listed.",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_list.add_argument("--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_list.add_argument(
        "-m", "--metadata",
        default="title,fileSize,id,md5Checksum",
        help="""Comma separated list of metadata keys to display.
             See
             https://developers.google.com/drive/v2/reference/files#resource""")
    parser_list.add_argument(
        "-d", "--depth",
        default=0,
        type=int,
        help="""Depth of a recursive listing of files in a folder. 0 only
             lists the file or folder itself. Depth must be >= 0. Using a large
             -d value can be slow for deeply nested directory hierarchies with many
             files.""")
    parser_list.add_argument(
        "-a", "--all",
        default=False,
        action="store_true",
        help="""List all files except for trashed. Much faster than specifying
                a -d value < 0.""")
    parser_list.add_argument(
        "-i", "--id",
        default=[],
        action="append",
        help="""File or folder ID. If not specified listing starts at Google
                Drive root. - to read a list of IDs from STDIN.""")
    parser_list.set_defaults(func=cli_list)

    # Download
    parser_download = subparsers.add_parser(
        "download", 
        help="Download files from Google Drive",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_download.add_argument(
        "--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_download.add_argument(
        "-i", "--id",
        default=[],
        action="append",
        help="File ID. - to read a list of IDs from STDIN.")
    parser_download.add_argument(
        "-n", "--no_checksum",
        default=False,
        action="store_true",
        help="Skip MD5 checksum verification after download")
    parser_download.add_argument(
        "-e", "--excludes",
        default=[],
        action="append",
        help="""Files with titles matching these Python regular expression
             pattern will be excluded. Does not apply to folders. (See
             --exclude_folders).""")
    parser_download.add_argument(
        "-v", "--invert_excludes",
        default=False,
        action="store_true",
        help="""Change exclude rules to become include rules""")
    parser_download.add_argument(
        "--exclude_folders",
        action="store_true",
        default=False,
        help="""Apply exclude rules to folder titles in addition to file titles""")
    parser_download.add_argument(
        "target",
        help="Destination directory")
    parser_download.set_defaults(func=cli_download)

    # Upload
    parser_upload = subparsers.add_parser(
        "upload", 
        help="Upload files to Google Drive",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_upload.add_argument(
        "--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_upload.add_argument(
        "-p", "--parent",
        default="root",
        metavar="ID",
        help="""Parent ID, i.e. containing folder ID. If not specified
             file will be placed in root folder.""")
    parser_upload.add_argument(
        "-n", "--no_checksum",
        default=False,
        action="store_true",
        help="Skip MD5 checksum verification after upload")
    parser_upload.add_argument(
        "-t", "--title",
        help="""Title for file/folder. Must be specified if folder is '.' or
             '..'""")
    parser_upload.add_argument(
        "-e", "--excludes",
        default=[],
        action="append",
        help="""Files with titles matching these Python regular expression
             pattern will be excluded. Does not apply to folders. (See
             --exclude_folders).""")
    parser_upload.add_argument(
        "-v", "--invert_excludes",
        default=False,
        action="store_true",
        help="""Change exclude rules to become include rules""")
    parser_upload.add_argument(
        "--exclude_folders",
        default=False,
        action="store_true",
        help="""Apply exclude rules to folder titles in addition to file titles""")
    parser_upload.add_argument(
        "files",
        nargs="+",
        help="Files/folders to upload")
    parser_upload.set_defaults(func=cli_upload)

    # Transfer ownership
    parser_transfer = subparsers.add_parser(
        "transfer", 
        help="""Transfer ownership of files or folders to a different account
        within the same domain""",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_transfer.add_argument(
        "--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_transfer.add_argument(
        "-i", "--id",
        default=[],
        action="append",
        help="File ID. - to read a list of IDs from STDIN.")
    parser_transfer.add_argument(
        "-e",
        "--email",
        help="""Google Apps account email address of new owner""")
    parser_transfer.set_defaults(func=cli_transfer_ownership)

    args = parser.parse_args()
    if args.subcommand_name != "version":
        args.drive = create_GoogleDrive()  # add GoogleDrive
        if args.verbose:
            verbose()
    args.func(args)

def cli_list(args):
    metadata_list = args.metadata.split(",")
    ids = parse_id_args(args.id)
    gdcp = Gdcp(args.drive)
    if args.depth < 0:
        error("list -d must be >= 0")
    if args.all:  # start at root
        gdcp.list_all_files(metadata_list)
    else:
        if len(ids) == 0:
            ids.append("root")
        gdcp.list(ids=ids, metadata=metadata_list, depth=args.depth)

def cli_download(args):
    gdcp = Gdcp(args.drive, excludes=args.excludes, include=args.invert_excludes,
        exclude_folders=args.exclude_folders)
    ids = parse_id_args(args.id)
    gdcp.download(ids=ids, checksum=not args.no_checksum, root=args.target)
    if gdcp.failed():
        gdcp.print_failed()
        sys.exit(1)
    else:
        log.info("Downloaded %i files and folders" % len(gdcp.files))

def cli_upload(args):
    gdcp = Gdcp(args.drive, excludes=args.excludes, include=args.invert_excludes,
        exclude_folders=args.exclude_folders)
    for f in args.files:
        if os.path.islink(f):
            error("Uploading symbolic links not supported")
    gdcp.upload(paths=args.files, title=args.title, parent=args.parent,
        checksum=not args.no_checksum)
    if gdcp.failed():
        gdcp.print_failed()
        sys.exit(1)
    else:
        log.info("Uploaded %i files and folders" % len(gdcp.files))

def cli_transfer_ownership(args):
    gdcp = Gdcp(args.drive)
    ids = parse_id_args(args.id)
    gdcp.transfer_ownership(ids, args.email)
    log.info("Transferred ownership for %i files" % len(gdcp.files))

def cli_version(args):
    print("%s version %s" % (PROJ, VERSION))

if __name__ == "__main__":
    main()

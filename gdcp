#!/usr/bin/env python

import os
import sys
import signal
import logging
from logging import info, warning
import re
import subprocess
import datetime
import time
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
import mimetypes
import httplib2
import ssl
import apiclient.errors
import apiclient.http
import oauth2client.client
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

VERSION = "0.5.6"
PROJ = "gdcp"  # name of this project
CONFIG_DIR = os.path.join(os.environ["HOME"], "." + PROJ)
CHUNKSIZE = 2 ** 20 * 250  # 250 MiB chunks

log = logging.getLogger(PROJ)
googleapi = logging.getLogger("googleapiclient.discovery")
oauth2_client = logging.getLogger("oauth2client.client")
oauth2_util = logging.getLogger("oauth2client.util")

# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
def main():
    configure_logging()
    configure_signals()
    do_setup()
    cli()

# -----------------------------------------------------------------------------
# Configuration functions
# -----------------------------------------------------------------------------
def configure_logging():
    fmt = "%(asctime)-25s %(levelname)-10s %(name)-26s: %(message)s"
    datefmt = "%m/%d/%Y %I:%M:%S %p"
    formatter = logging.Formatter(fmt=fmt, datefmt=datefmt)

    # Logger instances have been created globally
    google_handler = logging.StreamHandler()
    google_handler.setFormatter(formatter)
    googleapi.addHandler(google_handler)
    oauth2_client.addHandler(google_handler)
    oauth2_util.addHandler(google_handler)
    googleapi.setLevel(logging.WARNING)
    oauth2_client.setLevel(logging.WARNING)
    # oauth2_util is spitting out warnings like this
    # new_request() takes at most 1 positional argument (6 given)
    # set to error here to ignore
    oauth2_util.setLevel(logging.ERROR)

    main_handler = logging.StreamHandler()
    main_handler.setFormatter(formatter)
    log.addHandler(main_handler)
    log.setLevel(logging.INFO)

def verbose():
    googleapi.setLevel(logging.INFO)
    oauth2_client.setLevel(logging.INFO)
    oauth2_util.setLevel(logging.INFO)
    log.setLevel(logging.DEBUG)

def configure_signals():
    # signal handling
    # reset SIGINT and SIGPIPE
    # don't raise KeyboardInterrupt exception on SIGINT
    # dont' raise IOError on SIGPIPE
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)

def authorize(location=CONFIG_DIR):
    # Authentication
    settings_file = os.path.join(location, "settings.yaml")
    gauth = GoogleAuth(settings_file)
    gauth.CommandLineAuth()
    gauth.Authorize()
    return gauth

def do_setup(location=CONFIG_DIR):
    settings = os.path.join(location, "settings.yaml")
    client_secrets = os.path.join(location, "client_secrets.json")
    credentials = os.path.join(location, "credentials.json")

    settings_text = ["client_config_file: %s" % client_secrets]
    settings_text.append("get_refresh_token: True")
    settings_text.append("save_credentials: True")
    settings_text.append("save_credentials_backend: file")
    settings_text.append("save_credentials_file: %s" % credentials)

    if not os.path.exists(location):
        os.mkdir(location)
    elif not os.path.isdir(location):
        error("~/.%s already exists and is a file" % PROJ)

    if not os.path.exists(settings):
        try:
            with open(settings, "w") as fh:
                fh.write("\n".join(settings_text) + "\n")
        except (OSError, IOError) as e:
            error("Could not create settings file %s.  %s" %
                  (settings, e))

    if not os.path.exists(client_secrets):
        msg = ["%s not present" % client_secrets, ""]
        msg.append("- Visit https://console.developers.google.com/")
        msg.append("- Create a new project and select it")
        msg.append("- Under 'APIs' make sure the 'Drive API' is turned on")
        msg.append("- Under 'Credentials' create a new OAuth client ID")
        msg.append("  Choose 'Installed -> Other' for application type")
        msg.append("- Click 'Download JSON' to download the secrets file")
        msg.append("- Copy the secrets file to %s" % client_secrets)
        error("\n".join(msg))

    # If credentials have not been obtained, do so now
    if not os.path.exists(credentials):
        authorize()

# -----------------------------------------------------------------------------
# Google Drive interaction functions
# -----------------------------------------------------------------------------
def list_all_files(drive, metadata):
    query = "trashed = false"
    log.debug("query = '%s'" % query)
    files = drive.ListFile({"q": query}).GetList()
    for afile in files:
        print_one_file(afile, metadata)

def list_root(drive, metadata, depth):
    query = "trashed = false and 'root' in parents"
    log.debug("query = '%s'" % query)
    files = drive.ListFile({"q": query}).GetList()
    for subfile in files:
        print_one_file(subfile, metadata)
        if is_google_folder(subfile):
            list_file(drive, subfile, metadata, depth-1)

def list_file_by_id(drive, file_id, metadata, depth):
    afile = drive.CreateFile({"id": file_id})
    list_file(drive, afile, metadata, depth)

"""Print file listings starting at and including afile.

If depth == 0, only print this file.  If depth != 0 and afile is a folder,
call list_file on each child with depth-1.  Essentially, descend to the bottom
of the directory tree if depth < 0, stop if depth = 0, and continue for
depth steps if depth > 0.
"""
def list_file(drive, curfile, metadata, depth):
    if depth >= 0:
        if is_google_folder(curfile):
            query = "trashed = false and '%s' in parents" % curfile["id"]
            log.debug("query = '%s'" % query)
            files = drive.ListFile({"q": query}).GetList()
            for subfile in files:
                print_one_file(subfile, metadata)
                if is_google_folder(subfile):
                    list_file(drive, subfile, metadata, depth-1)
        else:
            print_one_file(curfile, metadata)

def download_by_file_id(drive, file_id, checksum=True, root="."):
    downfile = drive.CreateFile({"id": file_id})
    failed = download_file(drive, downfile, checksum=checksum, root=root)
    return failed
 
def download_file(drive, afile, checksum=True, root="."):
    # Track failed downloads or MD5 checksum verifications
    failed = {"HTTP": [], "MD5": []}

    if not os.path.exists(root):
        create_local_folder(root)
        
    if is_google_folder(afile):
        folder_path = os.path.join(root, afile["title"])
        folder_path = de_duplicate_path_name(folder_path)
        log.info("Creating folder %s" % folder_path)
        create_local_folder(folder_path)
        query = "'%s' in parents and trashed = false" % afile["id"]
        file_list = drive.ListFile({"q": query}).GetList()
        for onefile in file_list:
            failed_file = download_file(
                drive, onefile, checksum=checksum, root=folder_path)
            failed["HTTP"].extend(failed_file["HTTP"])
            failed["MD5"].extend(failed_file["MD5"])
    else:
        downpath = os.path.join(root, afile["title"])
        downpath = de_duplicate_path_name(downpath)
        file_size = int(afile["fileSize"])
        url = afile["downloadUrl"]
        log.info("Downloading %s, size = %i, md5 = %s" % 
                 (downpath, file_size, afile["md5Checksum"]))
        if url is None or len(url) == 0:
            error("%s is not downloadable" % afile["title"])

        retry_limit = 6
        retries = 0
        t1 = datetime.datetime.now()
        bytes_received = 0
        bytes_start = 0
        bytes_end = bytes_end = min(file_size - 1, bytes_received + CHUNKSIZE)
        bytes_end = max(bytes_end, 0)  # for empty files bytes_end might be -1
        prev_progress = 0.00

        with open(downpath, "wb") as fh:
            while bytes_received < file_size:
                headers = {
                    "range": "bytes=%i-%i" % (bytes_start, bytes_end),
                    "content-type": "application/octet-stream"
                }

                response, content = drive.auth.service._http.request(
                    url, method="GET", headers=headers)

                if response.status in [206, 200]:
                    fh.write(content)
                    log.debug(
                        "Downloaded bytes %i-%i with status %s" %
                        (bytes_start, bytes_end, response.status))
                    bytes_received += CHUNKSIZE
                    bytes_start = bytes_received + 1
                    bytes_end = min(file_size - 1, bytes_received + CHUNKSIZE)
                    # For empty files bytes_end might be -1
                    bytes_end = max(bytes_end, 0)

                    try:
                        cur_progress = float(bytes_received) / file_size * 100
                    except ZeroDivisionError:
                        cur_progress = 100.00
                    # Only print progress every 5% or greater
                    if cur_progress - prev_progress > 5.0 and cur_progress < 100.00:
                        t_tmp = datetime.datetime.now()
                        rate = calc_transfer_rate(t1, t_tmp, bytes_received)
                        log.info("Downloaded %.02f%% (%.02fMB/s)" %
                                 (cur_progress, rate))
                        prev_progress = cur_progress
                else:
                    if retries < retry_limit:
                        delay = 2 ** retries
                        log.warning("HTTP error %i, retrying bytes %i-%i in %is" %
                                    (response.status, bytes_start, bytes_end, delay))
                        retries += 1
                        time.sleep(delay)
                    else:
                        log.warning("HTTP error %i, aborting" % response.status)
                        failed["HTTP"] = [
                            {
                                "google_file": afile,
                                "root": root
                            }
                        ]
                        fh.close()
                        os.remove(downpath)
                        break

        t2 = datetime.datetime.now()
        rate = calc_transfer_rate(t1, t2, file_size)
        log.info("Downloaded 100.00%%.  %i bytes in %s (%.02fMB/s)" % 
             (file_size, format_timedelta(t1, t2), rate))

        if len(failed["HTTP"]) == 0:
            if checksum:
                if not check_md5(downpath, afile["md5Checksum"]):
                    failed["MD5"] = [
                        {
                            "google_file": afile,
                            "root": root
                        }
                    ]

    return failed

# Test upload_file error recovery by forcing some file uploads to fail
#upload_fails = {"count": 8}

def upload_file(drive, file_path, title=None, description=None, parent_id=None,
                checksum=True):
    if title is None:
        title = title_from_path(file_path)
    mimetype = guess_mimetype(file_path)
    media_body = create_media_body(file_path, CHUNKSIZE, mimetype)
    body = create_body(title, description, mimetype, parent_id)
    file_size = os.path.getsize(file_path)

    log.info("Uploading %s, mimeType=%s" % (file_path, body["mimeType"]))
    t1 = datetime.datetime.now()

    # Track failed uploads or MD5 checksum verifications
    failed = {"HTTP": [], "MD5": []}

    complete_retries = 0
    resumable_retries = 0
    retry_limit = 6
    response = None

    if file_size == 0:
        request = drive.auth.service.files().insert(body=body)
        while response is None:
            try:
                response = request.execute()
            except apiclient.errors.HttpError as e:
                if e.resp.status in [404, 410, 500, 502, 503, 504]:
                    if complete_retries < retry_limit:
                        # start upload all over
                        delay = 2 ** complete_retries
                        log.warning("HTTP error %i, restarting upload in %is" %
                                    (e.resp.status, delay))
                        complete_retries += 1
                        time.sleep(delay)
                        request = drive.auth.service.files().insert(body=body)
                    else:
                        # No more complete retries left, abort
                        msg = "HTTP error %i.  " % e.resp.status
                        msg += "Too many retries, aborting"
                        log.warning(msg)
                        failed["HTTP"] = [
                            {
                                "drive": drive,
                                "file_path": file_path,
                                "parent_id": parent_id,
                                "title": title,
                                "description": description,
                                "checksum": checksum
                            }
                        ]
                        break
                else:
                    # Don't know how to handle this error, abort
                    log.warning("HTTP error %i, aborting" % e.resp.status)
                    failed["HTTP"] = [
                        {
                            "drive": drive,
                            "file_path": file_path,
                            "parent_id": parent_id,
                            "title": title,
                            "description": description,
                            "checksum": checksum
                        }
                    ]
                    break
            except httplib2.HttpLib2Error as e:
                # Don't know how to handle this error, abort
                log.warning("Error %s, aborting" % e)
                failed["HTTP"] = [
                    {
                        "drive": drive,
                        "file_path": file_path,
                        "parent_id": parent_id,
                        "title": title,
                        "description": description,
                        "checksum": checksum
                    }
                ]
                break
            except (KeyError, ssl.SSLError) as e:
                if complete_retries < retry_limit:
                    # start upload all over
                    delay = 2 ** complete_retries
                    log.warning("Error %s, restarting upload in %is" %
                                (e, delay))
                    prev_progress = 0.00
                    resumable_retries = 0
                    bytes_sent = 0
                    complete_retries += 1
                    time.sleep(delay)
                    request = drive.auth.service.files().insert(body=body)
                else:
                    # No more complete retries left, abort
                    log.warning("Error %s. Too many retries, aborting" % e)
                    failed["HTTP"] = [
                        {
                            "drive": drive,
                            "file_path": file_path,
                            "parent_id": parent_id,
                            "title": title,
                            "description": description,
                            "checksum": checksum
                        }
                    ]
                    break
    else:
        request = drive.auth.service.files().insert(
            body=body,
            media_body=media_body)
        prev_progress = 0.00
        bytes_sent = 0

        while response is None:
            try:
                #if upload_fails["count"]:
                #    upload_fails["count"] -= 1
                #    raise Exception("fake exception")

                status, response = request.next_chunk()

                if status:
                    cur_progress = status.progress() * 100
                    bytes_sent = min(bytes_sent + CHUNKSIZE, file_size)
                    # Only print progress every 5% or greater
                    diff_progress = cur_progress - prev_progress
                    if diff_progress > 5.0 and cur_progress < 100.00:
                        t_tmp = datetime.datetime.now()
                        rate = calc_transfer_rate(t1, t_tmp, bytes_sent)
                        log.info("Uploaded %.02f%% (%.02fMB/s)" % 
                                 (cur_progress, rate))
                        prev_progress = cur_progress
            #except Exception as e:
            except apiclient.errors.HttpError as e:
                if e.resp.status in [404, 410]:
                    if complete_retries < retry_limit:
                        # start upload all over
                        delay = 2 ** complete_retries
                        log.warning("HTTP error %i, restarting upload in %is" %
                                    (e.resp.status, delay))
                        prev_progress = 0.00
                        resumable_retries = 0
                        bytes_sent = 0
                        complete_retries += 1
                        time.sleep(delay)
                        request = drive.auth.service.files().insert(
                            body=body,
                            media_body=media_body)
                    else:
                        # No more complete retries left, abort
                        msg = "HTTP error %i.  " % e.resp.status
                        msg += "Too many retries, aborting"
                        log.warning(msg)
                        failed["HTTP"] = [
                            {
                                "drive": drive,
                                "file_path": file_path,
                                "parent_id": parent_id,
                                "title": title,
                                "description": description,
                                "checksum": checksum
                            }
                        ]
                        break
                elif e.resp.status in [500, 502, 503, 504]:
                #elif False:
                    # Retry this chunk after a delay
                    if resumable_retries > retry_limit:
                        # No more retries left, abort
                        msg = "HTTP error %i.  " % e.resp.status
                        msg += "Too many retries, aborting"
                        log.warning(msg)
                        failed["HTTP"] = [
                            {
                                "drive": drive,
                                "file_path": file_path,
                                "parent_id": parent_id,
                                "title": title,
                                "description": description,
                                "checksum": checksum
                            }
                        ]
                        break
                    else:
                        delay = 2 ** resumable_retries
                        log.warning("HTTP error %i, retrying in %is ..." %
                                    (e.resp.status, delay))
                        time.sleep(delay)
                        resumable_retries += 1
                else:
                    # Don't know how to handle this error, abort
                    log.warning("HTTP error %i, aborting" % e.resp.status)
                    failed["HTTP"] = [
                        {
                            "drive": drive,
                            "file_path": file_path,
                            "parent_id": parent_id,
                            "title": title,
                            "description": description,
                            "checksum": checksum
                        }
                    ]
                    break
            except httplib2.HttpLib2Error as e:
                # Don't know how to handle this error, abort
                log.warning("Error %s, aborting" % e)
                failed["HTTP"] = [
                    {
                        "drive": drive,
                        "file_path": file_path,
                        "parent_id": parent_id,
                        "title": title,
                        "description": description,
                        "checksum": checksum
                    }
                ]
                break
            except (KeyError, ssl.SSLError) as e:
                if complete_retries < retry_limit:
                    # start upload all over
                    delay = 2 ** complete_retries
                    log.warning("Error %s, restarting upload in %is" %
                               (e, delay))
                    prev_progress = 0.00
                    resumable_retries = 0
                    bytes_sent = 0
                    complete_retries += 1
                    time.sleep(delay)
                    request = drive.auth.service.files().insert(
                        body=body,
                        media_body=media_body)
                else:
                    # No more complete retries left, abort
                    log.warning("Error %s. Too many retries, aborting" % e)
                    failed["HTTP"] = [
                        {
                            "drive": drive,
                            "file_path": file_path,
                            "parent_id": parent_id,
                            "title": title,
                            "description": description,
                            "checksum": checksum
                        }
                    ]
                    break

    if len(failed["HTTP"]) == 0:
        t2 = datetime.datetime.now()
        rate = calc_transfer_rate(t1, t2, file_size)
        log.info("Uploaded 100.00%%.  %i bytes in %s (%.02fMB/s)" % 
                 (file_size, format_timedelta(t1, t2), rate))

        if checksum:
            if not check_md5(file_path, response["md5Checksum"]):
                failed["MD5"] = [
                    {
                        "drive": drive,
                        "file_path": file_path,
                        "parent_id": parent_id,
                        "title": title,
                        "description": description,
                        "checksum": checksum
                    }
                ]

    return failed

def upload_folder(drive, folder_path, title=None, description=None,
                  parent_id=None, checksum=True):
    if title is None:
        title = title_from_path(folder_path)
    folder_id = create_google_folder(drive, title, description, parent_id)

    allfiles = path_join(folder_path, os.listdir(folder_path))
    files = file_filter(allfiles)
    dirs =  dir_filter(allfiles)
    
    # Track failed uploads or MD5 checksum verifications
    all_failed = {"HTTP": [], "MD5": []}

    for onefile in files:
        failed = upload_file(
            drive, onefile, parent_id=folder_id, checksum=checksum)
        all_failed["HTTP"].extend(failed["HTTP"])
        all_failed["MD5"].extend(failed["MD5"])
    for subdir in dirs:
        failed = upload_folder(
            drive, subdir, parent_id=folder_id, checksum=checksum)
        all_failed["HTTP"].extend(failed["HTTP"])
        all_failed["MD5"].extend(failed["MD5"])

    return all_failed

# -----------------------------------------------------------------------------
# Utility Functions
# -----------------------------------------------------------------------------
def print_one_file(afile, metadata):
    line = []
    for meta in metadata:
        try:

            line.append("%s: %s" % (meta, remove_r_n(afile[meta])))
        except KeyError:
            # Some files don't have common properties
            # e.g. folders don't have a fileSize
            # We don't want to throw an error for these cases
            # error("Invalid metadata property " +
            #       "'%s' in -m string '%s' " % (meta, metadata) +
            #       "for file %s" % afile['title'])
            pass
    print ", ".join(line)

def remove_r_n(some_string):
    some_string = some_string.replace("\r", "^M")
    some_string = some_string.replace("\n", "^M")
    return some_string

def create_media_body(file_path, chunksize, mimetype):
    media_body = apiclient.http.MediaFileUpload(
        file_path,
        chunksize=chunksize,
        resumable=True,
        mimetype=mimetype)
    return media_body

def create_body(title, description=None, mimetype=None, parent_id=None):
    body = {"title": title}
    if mimetype is None:
        body["mimeType"] = "application/vnd.google-apps.folder"
    else:
        body["mimeType"] = mimetype
    if description is not None:
        body["description"] = description
    if parent_id is not None:
        body["parents"] = [{"id": parent_id}]
    return body

def guess_mimetype(file_path):
    mimetype = mimetypes.guess_type(file_path)[0]
    if mimetype is None:
        mimetype = "application/octet-stream"
    return mimetype

def check_md5(file_path, response_md5):
    log.info("Calculating MD5 checksum for %s" % file_path)
    try:
        output = subprocess.check_output(["openssl", "md5", file_path],
                                         stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        error("MD5 calculation exited with an error: '%s'" %
              output.rstrip())
    md5 = output.split()[-1]
    if md5 == response_md5:
        log.info("MD5 OK.  %s (local) == %s" %
                 (md5, response_md5))
        passed = True
    else:
        passed = False
        log.warning("MD5 failed.  %s (local) != %s" %
                    (md5, response_md5))
    return passed

# Return iterable of items in files joined to join_path
def path_join(join_path, files):
    return map(lambda x: os.path.join(join_path, x), files)

# Return iterable of files which are not symbolic links
def file_filter(files):
    return filter(
        lambda x: not os.path.islink(x) and os.path.isfile(x),
        files)

# Return iterable of directories which are not symbolic links
def dir_filter(files):
    return filter(
        lambda x: not os.path.islink(x) and os.path.isdir(x),
        files)

"""Get a sanitized title for a file or folder.
"""
def title_from_path(file_path):
    file_path = os.path.abspath(file_path)  # resolve things like /foo/../foo

    # Special case for root
    if file_path == "/":
        return "/"

    # Remove trailing "/"s
    i = len(file_path) - 1
    while file_path[i] == "/":
        i -= 1
    file_path = file_path[:i+1]

    title = file_path.split("/")[-1]
    return title

"""Create a sensible new path name when old name is a duplicate.
"""
def de_duplicate_path_name(old_path):
    if not os.path.exists(old_path):
        return old_path
        
    delimiter = "_duplicate_"
    old_path = os.path.normpath(old_path)
    head, tail = os.path.split(old_path)
    old_tail_atoms = tail.split(delimiter)

    if delimiter in tail and old_tail_atoms[-1].isdigit():
        i = old_tail_atoms[-1]
        new_tail = tail[0:-len(i)] + str(int(i) + 1)
    else:
        new_tail = tail + delimiter + '1'

    new_path = os.path.join(head, new_tail)
   
    if os.path.exists(new_path):
        new_path = de_duplicate_path_name(new_path)
    
    return new_path

        
"""Create folder in Google Drive
"""
def create_google_folder(drive, title, description=None, parent_id=None):
    body = create_body(title, description, None, parent_id)
    log.debug("About to create folder %s" % body)
    folder = drive.auth.service.files().insert(body=body).execute()
    log.info("Created folder %s" % title)
    return folder["id"]

"""Create a local folder
"""
def create_local_folder(folder_path):
    try:
        os.makedirs(folder_path)
    except OSError as e:
        error("Could not create directory %s. Perhaps it already exists."\
              % folder_path)

def chdir(folder_path):
    try:
        os.chdir(folder_path)
    except OSError as e:
        error("Could not change to directory %s" % folder_path)

def error(msg):
    log.error(msg)
    sys.exit(1)

def format_timedelta(t1, t2):
    delta = t2 - t1
    delta_s = delta.total_seconds()
    elapse = []

    days = delta_s / (24 * 60 * 60)
    if int(days) > 0:
        elapse.append("%id" % int(days))
        delta_s -= days * 24 * 60 * 60

    hours = delta_s / (60 * 60)
    if int(hours):
        elapse.append("%ih" % int(hours))
        delta_s -= hours * 60 * 60

    minutes = delta_s / 60
    if int(minutes):
        elapse.append("%im" % int(minutes))
        delta_s -= minutes * 60

    seconds = delta_s
    elapse.append("%.02fs" % seconds)

    return "".join(elapse)

"""Return transfer rate for transfer of file_size bytes between t1 and t2
"""
def calc_transfer_rate(t1, t2, file_size):
    delta = t2 - t1
    delta_s = delta.total_seconds()
    try:
        rate = float(file_size) / delta_s / 10**6  # MB/s
    except ZeroDivisionError:
        rate = 0.00
    return rate

def is_google_folder(afile):
    return afile["mimeType"] == "application/vnd.google-apps.folder"


# -----------------------------------------------------------------------------
# Command-line interface functions
# -----------------------------------------------------------------------------
"""Parse command-line options.
"""
def cli():
    parser = ArgumentParser(
        description="Google Drive command-line interface",
        formatter_class=ArgumentDefaultsHelpFormatter)

    subparsers = parser.add_subparsers(
        dest="subcommand_name",
        help="Sub-command help")

    # Version
    parser_version = subparsers.add_parser(
        "version", 
        help="Print the semantic version number",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_version.set_defaults(func=cli_version)

    # List
    parser_list = subparsers.add_parser(
        "list", 
        help="List files in Google Drive.  Trashed files are listed.",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_list.add_argument("--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_list.add_argument(
        "-m", "--metadata",
        default="title,fileSize,id,md5Checksum",
        help="""Comma separated list of metadata keys to display.
             See
             https://developers.google.com/drive/v2/reference/files#resource""")
    parser_list.add_argument(
        "-d", "--depth",
        default=0,
        type=int,
        help="""Depth of a recursive listing of files in a folder.  0 only
             lists the file or folder itself.  Depth must be >= 0.  Using a large
             -d value can be slow for deeply nested directory hierarchies with many
             files.""")
    parser_list.add_argument(
        "-a", "--all",
        default=False,
        action="store_true",
        help="""List all files except for trashed.  Much faster than specifying
                a -d value < 0.""")
    parser_list.add_argument(
        "-i", "--id",
        help="""File or folder ID.  If not specified listing starts at Google
                Drive root.""")
    parser_list.set_defaults(func=cli_list_files)

    # Download
    parser_download = subparsers.add_parser(
        "download", 
        help="Download a file from Google Drive",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_download.add_argument(
        "--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_download.add_argument(
        "-i", "--id",
        help="File ID")
    parser_download.add_argument(
        "-n", "--no_checksum",
        default=False,
        action="store_true",
        help="Skip MD5 checksum verification after download")
    parser_download.add_argument(
        "target",
        help="Destination directory")
    parser_download.set_defaults(func=cli_download_file)

    # Upload
    parser_upload = subparsers.add_parser(
        "upload", 
        help="Upload a file or folder to Google Drive",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_upload.add_argument(
        "--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_upload.add_argument(
        "-p", "--parent",
        metavar="ID",
        help="""Parent ID, i.e. containing folder ID.  If not specified
             file will be placed in root folder.""")
    parser_upload.add_argument(
        "-n", "--no_checksum",
        default=False,
        action="store_true",
        help="Skip MD5 checksum verification after upload")
    parser_upload.add_argument(
        "-t", "--title",
        help="""Title for file/folder.  Must be specified if folder is '.' or
             '..'""")
    parser_upload.add_argument(
        "-d", "--description",
        help="Description for file/folder")
    parser_upload.add_argument(
        "file",
        help="File/folder to upload")
    parser_upload.set_defaults(func=cli_upload_file)

    args = parser.parse_args()
    if args.subcommand_name != "version":
        args.drive = GoogleDrive(authorize())  # add GoogleDrive
        if args.verbose:
            verbose()
    args.func(args)

def cli_list_files(args):
    metadata_list = args.metadata.split(",")
    if args.depth < 0:
        error("list -d must be >= 0")
    if args.id is None:  # start at root
        if args.all:
            list_all_files(args.drive, metadata_list)
        else:
            list_root(args.drive, metadata_list, args.depth)
    else:
        if args.all:
            error("list -a not compatible with -d")
        list_file_by_id(args.drive, args.id, metadata_list, args.depth)

def cli_download_file(args):
    failed = download_by_file_id(args.drive, args.id, not args.no_checksum,
                                 args.target)

    # If any files failed to download, retry, then print the
    # files if any still don't download
    bad_downloads = []
    bad_md5 = []
    if len(failed["HTTP"]) > 0:
        log.info("%s file downloads failed" % len(failed["HTTP"]))
        for f in failed["HTTP"]:
            downpath = os.path.join(f["root"], f["google_file"]["title"])
            log.info("Retrying " + downpath)
            new_fail = download_file(
                args.drive, f["google_file"],
                checksum=not args.no_checksum, root=f["root"])
            if len(new_fail["HTTP"]) > 0:
                # Still couldn't download the file, print an error report
                bad_downloads.append(downpath)
            elif len(new_fail["MD5"]) > 0:
                # Download was successful, but MD5 failed
                failed["MD5"].extend(new_fail["MD5"])

        if len(bad_downloads) == 0:
            log.info("All file download retries were successful")
        else:
            log.warning("%i files failed to download:\n%s" %
                        (len(bad_downloads), "\n".join(bad_downloads)))

    bad_md5 = map(
        lambda x: os.path.join(x["root"], x["google_file"]["title"]),
        failed["MD5"])
    if len(bad_md5) > 0:
        log.warning("%i files failed MD5 verification\n%s" %
                    (len(failed["MD5"]), "\n".join(bad_md5)))

    if len(bad_downloads) == 0 and len(bad_md5) == 0:
        log.info("All downloads were successful")

def cli_upload_file(args):
    if os.path.islink(args.file):
        error("Uploading symbolic links not supported")
    if os.path.isdir(args.file):
        folder = title_from_path(args.file)
        if folder in [".", "..", "/"]:
            if not args.title:
                error("-t must be supplied if folder is '.', '..', or '/'")
        failed = upload_folder(args.drive, args.file, args.title,
                               args.description, args.parent,
                               not args.no_checksum)
    else:
        failed = upload_file(args.drive, args.file, args.title,
                             args.description, args.parent,
                             not args.no_checksum)

    # If any files failed to upload, retry, then print the
    # files if any still don't upload
    bad_uploads = []
    bad_md5 = []
    if len(failed["HTTP"]) > 0:
        log.info("%s file uploads failed" % len(failed["HTTP"]))
        for f in failed["HTTP"]:
            log.info("Retrying " + f["file_path"])
            new_fail = upload_file(
                f["drive"], f["file_path"], f["title"], f["description"],
                f["parent_id"], f["checksum"])

            if len(new_fail["HTTP"]) > 0:
                # Still couldn't upload the file, print an error report
                bad_uploads.append(new_fail["HTTP"][0]["file_path"])
            elif len(new_fail["MD5"]) > 0:
                # Upload was successful, but MD5 failed
                failed["MD5"].extend(new_fail["MD5"])

        if len(bad_uploads) == 0:
            log.info("All file upload retries were successful")
        else:
            log.warning("%i files failed to upload:\n%s" %
                        (len(bad_uploads), "\n".join(bad_uploads)))

    bad_md5 = map(lambda x: x["file_path"], failed["MD5"])
    if len(bad_md5) > 0:
        log.warning("%i files failed MD5 verification\n%s" %
                    (len(failed["MD5"]), "\n".join(bad_md5)))

    if len(bad_uploads) == 0 and len(bad_md5) == 0:
        log.info("All uploads were successful")


def cli_version(args):
    print("%s version %s" % (PROJ, VERSION))

if __name__ == "__main__":
    main()

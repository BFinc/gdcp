#!/usr/bin/env python

import os
import sys
import signal
import logging
from logging import info, warning
import re
import subprocess
import datetime
import time
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
import mimetypes
import httplib2
import apiclient.errors
import apiclient.http
import oauth2client.client
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

VERSION = "0.4.3"
PROJ = "gdcp"  # name of this project
CONFIG_DIR = os.path.join(os.environ["HOME"], "." + PROJ)
CHUNKSIZE = 2 ** 20 * 250  # 250 MiB chunks

log = logging.getLogger(PROJ)
googleapi = logging.getLogger("googleapiclient.discovery")
oauth2_client = logging.getLogger("oauth2client.client")
oauth2_util = logging.getLogger("oauth2client.util")

# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
def main():
    configure_logging()
    configure_signals()
    do_setup()
    cli()

# -----------------------------------------------------------------------------
# Configuration functions
# -----------------------------------------------------------------------------
def configure_logging():
    fmt = "%(asctime)-25s %(levelname)-10s %(name)-26s: %(message)s"
    datefmt = "%m/%d/%Y %I:%M:%S %p"
    formatter = logging.Formatter(fmt=fmt, datefmt=datefmt)

    # Logger instances have been created globally
    google_handler = logging.StreamHandler()
    google_handler.setFormatter(formatter)
    googleapi.addHandler(google_handler)
    oauth2_client.addHandler(google_handler)
    oauth2_util.addHandler(google_handler)
    googleapi.setLevel(logging.WARNING)
    oauth2_client.setLevel(logging.WARNING)
    # oauth2_util is spitting out warnings like this
    # new_request() takes at most 1 positional argument (6 given)
    # set to error here to ignore
    oauth2_util.setLevel(logging.ERROR)

    main_handler = logging.StreamHandler()
    main_handler.setFormatter(formatter)
    log.addHandler(main_handler)
    log.setLevel(logging.INFO)

def verbose():
    googleapi.setLevel(logging.INFO)
    oauth2_client.setLevel(logging.INFO)
    oauth2_util.setLevel(logging.INFO)
    log.setLevel(logging.DEBUG)

def configure_signals():
    # signal handling
    # reset SIGINT and SIGPIPE
    # don't raise KeyboardInterrupt exception on SIGINT
    # dont' raise IOError on SIGPIPE
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)

def authorize(location=CONFIG_DIR):
    # Authentication
    settings_file = os.path.join(location, "settings.yaml")
    gauth = GoogleAuth(settings_file)
    gauth.CommandLineAuth()
    gauth.Authorize()
    return gauth

def do_setup(location=CONFIG_DIR):
    settings = os.path.join(location, "settings.yaml")
    client_secrets = os.path.join(location, "client_secrets.json")
    credentials = os.path.join(location, "credentials.json")

    settings_text = ["client_config_file: %s" % client_secrets]
    settings_text.append("get_refresh_token: True")
    settings_text.append("save_credentials: True")
    settings_text.append("save_credentials_backend: file")
    settings_text.append("save_credentials_file: %s" % credentials)

    if not os.path.exists(location):
        os.mkdir(location)
    elif not os.path.isdir(location):
        error("~/.%s already exists and is a file" % PROJ)

    if not os.path.exists(settings):
        try:
            with open(settings, "w") as fh:
                fh.write("\n".join(settings_text) + "\n")
        except (OSError, IOError) as e:
            error("Could not create settings file %s.  %s" %
                  (settings, e))

    if not os.path.exists(client_secrets):
        msg = ["%s not present" % client_secrets, ""]
        msg.append("- Visit https://console.developers.google.com/")
        msg.append("- Create a new project and select it")
        msg.append("- Under 'APIs' make sure the 'Drive API' is turned on")
        msg.append("- Under 'Credentials' create a new OAuth client ID")
        msg.append("  Choose 'Installed -> Other' for application type")
        msg.append("- Click 'Download JSON' to download the secrets file")
        msg.append("- Copy the secrets file to %s" % client_secrets)
        error("\n".join(msg))

    # If credentials have not been obtained, do so now
    if not os.path.exists(credentials):
        authorize()

# -----------------------------------------------------------------------------
# Google Drive interaction functions
# -----------------------------------------------------------------------------
def list_all_files(drive, metadata):
    query = "trashed = false"
    log.debug("query = '%s'" % query)
    files = drive.ListFile({"q": query}).GetList()
    for afile in files:
        print_one_file(afile, metadata)

def list_root(drive, metadata, depth):
    query = "trashed = false and 'root' in parents"
    log.debug("query = '%s'" % query)
    files = drive.ListFile({"q": query}).GetList()
    for subfile in files:
        print_one_file(subfile, metadata)
        if is_folder(subfile):
            list_file(drive, subfile, metadata, depth-1)

def list_file_by_id(drive, file_id, metadata, depth):
    afile = drive.CreateFile({"id": file_id})
    list_file(drive, afile, metadata, depth)

"""Print file listings starting at and including afile.

If depth == 0, only print this file.  If depth != 0 and afile is a folder,
call list_file on each child with depth-1.  Essentially, descend to the bottom
of the directory tree if depth < 0, stop if depth = 0, and continue for
depth steps if depth > 0.
"""
def list_file(drive, curfile, metadata, depth):
    if depth >= 0:
        if is_folder(curfile):
            query = "trashed = false and '%s' in parents" % curfile["id"]
            log.debug("query = '%s'" % query)
            files = drive.ListFile({"q": query}).GetList()
            for subfile in files:
                print_one_file(subfile, metadata)
                if is_folder(subfile):
                    list_file(drive, subfile, metadata, depth-1)
        else:
            print_one_file(curfile, metadata)

def download_by_file_id(drive, file_id, checksum=False, root="."):
    downfile = drive.CreateFile({"id": file_id})
    download_file(drive, downfile, checksum, root)
 
def download_file(drive, afile, checksum=False, root="."):
    if is_folder(afile):
        download_folder(drive, afile, root)
    else:
        downpath = os.path.join(root, afile["title"])
        file_size = int(afile["fileSize"])
        url = afile["downloadUrl"]
        log.info("Downloading %s, size = %i, md5 = %s" % 
                 (downpath, file_size, afile["md5Checksum"]))
        if url is None or len(url) == 0:
            error("%s is not downloadable" % afile["title"])
        #log.debug("Download url = " + url)

        t1 = datetime.datetime.now()
        bytes_received = -1  # so +1 in first loop iteration increments to 0
        prev_progress = 0.00
        with open(downpath, "wb") as fh:
            while bytes_received < file_size:
                bytes_start = bytes_received + 1
                bytes_end = min(file_size - 1, bytes_received + CHUNKSIZE)
                headers = {
                    "range": "bytes=%i-%i" % (bytes_start, bytes_end),
                    "content-type": "application/octet-stream"
                }
                resp, content = drive.auth.service._http.request(
                    url, method="GET", headers=headers)
                # TODO CTB: should we retry as with resumable uploads given
                # certain non-fatal errors?
                if resp.status in [206, 200]:
                    fh.write(content)
                    log.debug(
                        "Downloaded bytes %i-%i with status %s" %
                        (bytes_start, bytes_end, resp.status))
                    bytes_received += CHUNKSIZE
                    cur_progress = float(bytes_received) / file_size * 100
                    # Only print progress every 5% or greater
                    if cur_progress - prev_progress > 5.0 and cur_progress < 100.00:
                        t_tmp = datetime.datetime.now()
                        rate = calc_transfer_rate(t1, t_tmp, bytes_received)
                        log.info("Downloaded %.02f%% (%.02fMB/s)" %
                                 (cur_progress, rate))
                        prev_progress = cur_progress
                else:
                    fh.close()
                    os.remove(downpath)
                    error("An error occurred: %i" % resp.status)

        t2 = datetime.datetime.now()
        rate = calc_transfer_rate(t1, t2, file_size)
        log.info("Downloaded 100.00%%.  %i bytes in %s (%.02fMB/s)" % 
             (file_size, format_timedelta(t1, t2), rate))

        if checksum:
            check_md5(downpath, afile["md5Checksum"])

def download_folder(drive, gfolder, checksum=False, root="."):
    folder_path = os.path.join(root, gfolder["title"])
    log.info("Creating folder %s" % folder_path)
    create_lfolder(folder_path)
    query = "'%s' in parents and trashed = false" % gfolder["id"]
    file_list = drive.ListFile({"q": query}).GetList()
    for onefile in file_list:
        if is_folder(onefile):
            download_folder(drive, onefile, checksum, root=folder_path)
        else:
            download_file(drive, onefile, checksum, root=folder_path)

# Test upload_file error recovery by forcing some file uploads to fail
#upload_fails = {"count": 8}

def upload_file(drive, file_path, title=None, description=None, parent_id=None,
                checksum=False):
    if title is None:
        title = title_from_path(file_path)
    mimetype = guess_mimetype(file_path)
    media_body = create_media_body(file_path, CHUNKSIZE, mimetype)
    body = create_body(title, description, mimetype, parent_id)
    file_size = os.path.getsize(file_path)

    log.info("Uploading %s, mimeType=%s" % (file_path, body["mimeType"]))
    t1 = datetime.datetime.now()

    # Track failed uploads or MD5 checksum verifications
    failed = {"HTTP": [], "MD5": []}

    if file_size == 0:
        response = drive.auth.service.files().insert(body=body).execute()
    else:
        request = drive.auth.service.files().insert(
            body=body,
            media_body=media_body)
        response = None
        complete_retries = 0
        complete_retry_limit = 1
        resumable_retries = 0
        resumable_retry_limit = 6
        prev_progress = 0.00
        bytes_sent = 0

        while response is None:
            try:
                #if upload_fails["count"]:
                #    upload_fails["count"] -= 1
                #    raise Exception("fake exception")

                status, response = request.next_chunk()
                if status:
                    cur_progress = status.progress() * 100
                    bytes_sent = min(bytes_sent + CHUNKSIZE, file_size)
                    # Only print progress every 5% or greater
                    diff_progress = cur_progress - prev_progress
                    if diff_progress > 5.0 and cur_progress < 100.00:
                        t_tmp = datetime.datetime.now()
                        rate = calc_transfer_rate(t1, t_tmp, bytes_sent)
                        log.info("Uploaded %.02f%% (%.02fMB/s)" % 
                                 (cur_progress, rate))
                        prev_progress = cur_progress
            #except Exception as e:
            except apiclient.errors.HttpError as e:
                if e.resp.status in [404, 410]:
                    if complete_retries < complete_retry_limit:
                        # start upload all over
                        log.warning("HTTP error %d, restarting upload" %
                                    e.resp.status)
                        prev_progress = 0.00
                        resumable_retries = 0
                        bytes_sent = 0
                        complete_retries += 1
                        request = drive.auth.service.files().insert(
                            body=body,
                            media_body=media_body)
                        delay = 2 ** complete_retries
                        time.sleep(delay)
                elif e.resp.status in [500, 502, 503, 504]:
                #elif False:
                    # Retry this chunk after a delay
                    if resumable_retries > resumable_retry_limit:
                        # No more retries left, abort
                        log.warning("Too many retries, aborting")
                        failed["HTTP"] = [
                            {
                                "drive": drive,
                                "file_path": file_path,
                                "parent_id": parent_id,
                                "title": title,
                                "description": description,
                                "checksum": checksum
                            }
                        ]
                        break
                    else:
                        delay = 2 ** resumable_retries
                        log.warning("HTTP error %d, retrying in %ds ..." %
                                    (e.resp.status, delay))
                        time.sleep(delay)
                        resumable_retries += 1
                else:
                    # Don't know how to handle this error, abort
                    log.warning("HTTP error %i, aborting" % e.resp.status)
                    failed["HTTP"] = [
                        {
                            "drive": drive,
                            "file_path": file_path,
                            "parent_id": parent_id,
                            "title": title,
                            "description": description,
                            "checksum": checksum
                        }
                    ]
                    break

    if len(failed["HTTP"]) == 0:
        t2 = datetime.datetime.now()
        rate = calc_transfer_rate(t1, t2, file_size)
        log.info("Uploaded 100.00%%.  %i bytes in %s (%.02fMB/s)" % 
                 (file_size, format_timedelta(t1, t2), rate))

        if checksum:
            if not check_md5(file_path, response["md5Checksum"]):
                failed["MD5"] = [
                    {
                        "drive": drive,
                        "file_path": file_path,
                        "parent_id": parent_id,
                        "title": title,
                        "description": description,
                        "checksum": checksum
                    }
                ]

    return failed

def upload_folder(drive, folder_path, title=None, description=None,
                  parent_id=None, checksum=False):
    if title is None:
        title = title_from_path(folder_path)
    folder_id = create_gfolder(drive, title, description, parent_id)

    allfiles = path_join(folder_path, os.listdir(folder_path))
    files = file_filter(allfiles)
    dirs =  dir_filter(allfiles)
    
    # Track failed uploads or MD5 checksum verifications
    all_failed = {"HTTP": [], "MD5": []}

    for onefile in files:
        failed = upload_file(
            drive, onefile, parent_id=folder_id, checksum=checksum)
        all_failed["HTTP"].extend(failed["HTTP"])
        all_failed["MD5"].extend(failed["MD5"])
    for subdir in dirs:
        failed = upload_folder(
            drive, subdir, parent_id=folder_id, checksum=checksum)
        all_failed["HTTP"].extend(failed["HTTP"])
        all_failed["MD5"].extend(failed["MD5"])

    return all_failed

# -----------------------------------------------------------------------------
# Utility Functions
# -----------------------------------------------------------------------------
def print_one_file(afile, metadata):
    line = []
    for meta in metadata:
        try:

            line.append("%s: %s" % (meta, remove_r_n(afile[meta])))
        except KeyError:
            # Some files don't have common properties
            # e.g. folders don't have a fileSize
            # We don't want to throw an error for these cases
            # error("Invalid metadata property " +
            #       "'%s' in -m string '%s' " % (meta, metadata) +
            #       "for file %s" % afile['title'])
            pass
    print ", ".join(line)

def remove_r_n(some_string):
    some_string = some_string.replace("\r", "^M")
    some_string = some_string.replace("\n", "^M")
    return some_string

def create_media_body(file_path, chunksize, mimetype):
    media_body = apiclient.http.MediaFileUpload(
        file_path,
        chunksize=chunksize,
        resumable=True,
        mimetype=mimetype)
    return media_body

def create_body(title, description=None, mimetype=None, parent_id=None):
    body = {"title": title}
    if mimetype is None:
        body["mimeType"] = "application/vnd.google-apps.folder"
    else:
        body["mimeType"] = mimetype
    if description is not None:
        body["description"] = description
    if parent_id is not None:
        body["parents"] = [{"id": parent_id}]
    return body

def guess_mimetype(file_path):
    mimetype = mimetypes.guess_type(file_path)[0]
    if mimetype is None:
        mimetype = "application/octet-stream"
    return mimetype

def check_md5(file_path, response_md5):
    passed = False
    log.info("Calculating MD5 checksum for %s" % file_path)
    try:
        output = subprocess.check_output(["openssl", "md5", file_path],
                                         stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        error("MD5 calculation exited with an error: '%s'" %
              output.rstrip())
    md5 = output.split()[-1]
    if md5 == response_md5:
        log.info("MD5 OK.  %s (local) == %s" %
                 (md5, response_md5))
        passed = True
    else:
        log.warning("MD5 failed.  %s (local) != %s" %
                    (md5, response_md5))
    return passed

# Return iterable of items in files joined to join_path
def path_join(join_path, files):
    return map(lambda x: os.path.join(join_path, x), files)

# Return iterable of file which are not symbolic links
def file_filter(files):
    return filter(
        lambda x: not os.path.islink(x) and os.path.isfile(x),
        files)

# Return iterable of directories which are not symbolic links
def dir_filter(files):
    return filter(
        lambda x: not os.path.islink(x) and os.path.isdir(x),
        files)

"""Get a sanitized title for a file or folder.
"""
def title_from_path(file_path):
    file_path = os.path.abspath(file_path)  # resolve things like /foo/../foo

    # Special case for root
    if file_path == "/":
        return "/"

    # Remove trailing "/"s
    i = len(file_path) - 1
    while file_path[i] == "/":
        i -= 1
    file_path = file_path[:i+1]

    title = file_path.split("/")[-1]
    return title

"""Create folder in Google Drive
"""
def create_gfolder(drive, title, description=None, parent_id=None):
    body = create_body(title, description, None, parent_id)
    log.debug("About to create folder %s" % body)
    folder = drive.auth.service.files().insert(body=body).execute()
    log.info("Created folder %s" % title)
    return folder["id"]

"""Create a local folder
"""
def create_lfolder(folder_path):
    try:
        os.mkdir(folder_path)
    except OSError as e:
        error("Could not create directory %s" % folder_path)

def chdir(folder_path):
    try:
        os.chdir(folder_path)
    except OSError as e:
        error("Could not change to directory %s" % folder_path)

def error(msg):
    log.error(msg)
    sys.exit(1)

def format_timedelta(t1, t2):
    delta = t2 - t1
    delta_s = delta.total_seconds()
    elapse = []

    days = delta_s / (24 * 60 * 60)
    if int(days) > 0:
        elapse.append("%id" % int(days))
        delta_s -= days * 24 * 60 * 60

    hours = delta_s / (60 * 60)
    if int(hours):
        elapse.append("%ih" % int(hours))
        delta_s -= hours * 60 * 60

    minutes = delta_s / 60
    if int(minutes):
        elapse.append("%im" % int(minutes))
        delta_s -= minutes * 60

    seconds = delta_s
    elapse.append("%.02fs" % seconds)

    return "".join(elapse)

"""Return transfer rate for transfer of file_size bytes between t1 and t2
"""
def calc_transfer_rate(t1, t2, file_size):
    delta = t2 - t1
    delta_s = delta.total_seconds()
    try:
        rate = float(file_size) / delta_s / 10**6  # MB/s
    except ZeroDivisionError:
        rate = 0.00
    return rate

def is_folder(afile):
    return afile["mimeType"] == "application/vnd.google-apps.folder"

def is_file(onefile):
    return not is_folder(afile)

# -----------------------------------------------------------------------------
# Command-line interface functions
# -----------------------------------------------------------------------------
"""Parse command-line options.
"""
def cli():
    parser = ArgumentParser(
        description="Google Drive command-line interface",
        formatter_class=ArgumentDefaultsHelpFormatter)

    subparsers = parser.add_subparsers(
        dest="subcommand_name",
        help="Sub-command help")

    # Version
    parser_version = subparsers.add_parser(
        "version", 
        help="Print the semantic version number",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_version.set_defaults(func=cli_version)

    # List
    parser_list = subparsers.add_parser(
        "list", 
        help="List files in Google Drive.  Trashed files are listed.",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_list.add_argument("--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_list.add_argument(
        "-m", "--metadata",
        default="title,fileSize,id,md5Checksum",
        help="""Comma separated list of metadata keys to display.
             See
             https://developers.google.com/drive/v2/reference/files#resource""")
    parser_list.add_argument(
        "-d", "--depth",
        default=0,
        type=int,
        help="""Depth of a recursive listing of files in a folder.  0 only
             lists the file or folder itself.  Depth must be >= 0.  Using a large
             -d value can be slow for deeply nested directory hierarchies with many
             files.""")
    parser_list.add_argument(
        "-a", "--all",
        default=False,
        action="store_true",
        help="""List all files except for trashed.  Much faster than specifying
                a -d value < 0.""")
    parser_list.add_argument(
        "-i", "--id",
        help="""File or folder ID.  If not specified listing starts at Google
             Drive root.""")
    parser_list.set_defaults(func=cli_list_files)

    # Download
    parser_download = subparsers.add_parser(
        "download", 
        help="Download a file from Google Drive",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_download.add_argument("--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_download.add_argument(
        "-i", "--id",
        help="File ID")
    parser_download.set_defaults(func=cli_download_file)
    parser_download.add_argument(
        "-c", "--checksum",
        default=False,
        action="store_true",
        help="Verify download success by MD5 checksum after download"
    )

    # Upload
    parser_upload = subparsers.add_parser(
        "upload", 
        help="Upload a file or folder to Google Drive",
        formatter_class=ArgumentDefaultsHelpFormatter)
    parser_upload.add_argument("--verbose",
        default=False,
        action="store_true",
        help="Verbose logging output")
    parser_upload.add_argument(
        "-p", "--parent",
        metavar="ID",
        help="""Parent ID, i.e. containing folder ID.  If not specified
             file will be placed in root folder.""")
    parser_upload.add_argument(
        "-c", "--checksum",
        default=False,
        action="store_true",
        help="Verify upload success by MD5 checksum after upload"
    )
    parser_upload.add_argument(
        "-t", "--title",
        help="""Title for file/folder.  Must be specified if folder is '.' or
             '..'""")
    parser_upload.add_argument(
        "-d", "--description",
        help="Description for file/folder")
    parser_upload.add_argument(
        "file",
        help="File to upload")
    parser_upload.set_defaults(func=cli_upload_file)

    args = parser.parse_args()
    if args.subcommand_name != "version":
        args.drive = GoogleDrive(authorize())  # add GoogleDrive
        if args.verbose:
            verbose()
    args.func(args)

def cli_list_files(args):
    metadata_list = args.metadata.split(",")
    if args.depth < 0:
        error("list -d must be >= 0")
    if args.id is None:  # start at root
        if args.all:
            list_all_files(args.drive, metadata_list)
        else:
            list_root(args.drive, metadata_list, args.depth)
    else:
        if args.all:
            error("list -a not compatible with -d")
        list_file_by_id(args.drive, args.id, metadata_list, args.depth)

def cli_download_file(args):
    download_by_file_id(args.drive, args.id, args.checksum)
    log.info("Download successful")

def cli_upload_file(args):
    if os.path.islink(args.file):
        error("Uploading symbolic links not supported")
    if os.path.isdir(args.file):
        folder = title_from_path(args.file)
        if folder in [".", "..", "/"]:
            if not args.title:
                error("-t must be supplied if folder is '.', '..', or '/'")
        failed = upload_folder(args.drive, args.file, args.title,
                               args.description, args.parent, args.checksum)
    else:
        failed = upload_file(args.drive, args.file, args.title,
                             args.description, args.parent, args.checksum)

    # If any files failed to upload, retry, then print the
    # files if any still don't upload
    bad_uploads = []
    bad_md5 = []
    if len(failed["HTTP"]) > 0:
        log.info("%s file uploads failed" % len(failed["HTTP"]))
        for f in failed["HTTP"]:
            log.info("Retrying " + f["file_path"])
            new_fail = upload_file(
                f["drive"], f["file_path"], f["title"], f["description"],
                f["parent_id"], f["checksum"])

            if len(new_fail["HTTP"]) > 0:
                # Still couldn't upload the file, print an error report
                bad_uploads.append(new_fail["HTTP"][0]["file_path"])
            elif len(new_fail["MD5"]) > 0:
                # Upload was successful, but MD5 failed
                failed["MD5"].extend(new_fail["MD5"])

        if len(bad_uploads) == 0:
            log.info("All file upload retries were successful")
        else:
            log.warning("%i files failed to upload:\n%s" %
                        (len(bad_uploads), "\n".join(bad_uploads)))

    bad_md5 = map(lambda x: x["file_path"], failed["MD5"])
    if len(bad_md5):
        log.warning("%i files failed MD5 verification\n%s" %
                    (len(failed["MD5"]), "\n".join(bad_md5)))


def cli_version(args):
    print("%s version %s" % (PROJ, VERSION))

if __name__ == "__main__":
    main()
